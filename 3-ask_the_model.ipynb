{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6158c1dd",
   "metadata": {},
   "source": [
    "Here, we try the model on the test dataset, to know if it learned the definitions of the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919b9014",
   "metadata": {},
   "source": [
    "## 1 - Loads model and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3341b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "date=\"09_02_2025-14h_17min\" # here, put the name of the folder of the training sessions\n",
    "session_path = f\"../bucket/fine_tuning_acronym/sessions/results_{date}\"\n",
    "checkpoint_name = \"checkpoint-150\" # here, put the checkpoint name (inside the training session folder)\n",
    "\n",
    "model_path = os.path.join(session_path, \"model\", checkpoint_name)\n",
    "data_dir = \"../bucket/fine_tuning_acronym/data\"\n",
    "test_dir = os.path.join(session_path, \"tests\")\n",
    "\n",
    "if not os.path.exists(test_dir):\n",
    "    os.makedirs(test_dir)\n",
    "\n",
    "\n",
    "\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "print(f\"\"\"\n",
    "    Model will be loaded from : {model_path},\n",
    "    Datatype: {dtype},\n",
    "    Tests will be saved at : {test_dir}\n",
    "    Loads test data from : {data_dir}.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d128d811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads data for evaluation\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "path_test_dataset = os.path.join(data_dir, \"test_dataset.json\")\n",
    "print(f\"Loading eval data from : {path_test_dataset}\")\n",
    "\n",
    "with open(path_test_dataset, \"rt\") as f:\n",
    "    test_dataset = json.load(f)\n",
    "\n",
    "print(test_dataset[1]) # example of an element of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b14b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pl = pipeline(\"text-generation\", model=model_path, torch_dtype=dtype, do_sample=True, max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b932c076",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl(\"1+1 ?\", pad_token_id=pl.tokenizer.eos_token_id) # test model availability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08036c10",
   "metadata": {},
   "source": [
    "# 2 - Model evaluation\n",
    "\n",
    "Now that the model is trained, we can make an automatic evaluation of it. To do so, we first ask the model all the questions of our test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5034d1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_convs = [\n",
    "    [each_acro[\"conversation\"][0][0]] for each_acro in test_dataset\n",
    "]\n",
    "\n",
    "answers_raw = pl(all_test_convs) # ask all the questions\n",
    "\n",
    "print(json.dumps(answers_raw[0], indent=4)) # example of answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb61b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_dataset = []\n",
    "\n",
    "for k, each_answer in enumerate(answers_raw):\n",
    "    question = each_answer[0][\"generated_text\"][0][\"content\"]\n",
    "    answer = each_answer[0][\"generated_text\"][1][\"content\"]\n",
    "    acronym = test_dataset[k][\"acronym\"]\n",
    "    ground_truth = test_dataset[k][\"ground_truth\"]\n",
    "    expected_answer = test_dataset[k][\"conversation\"][0][1][\"content\"]\n",
    "    answer_dataset.append({\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "        \"expected_answer\": expected_answer,\n",
    "        \"ground_truth\": ground_truth,\n",
    "        \"acronym\": acronym\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dfe9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_dataset[1] # example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572e9f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_answer_dataset = os.path.join(test_dir, \"answer_dataset.json\")\n",
    "\n",
    "print(f\"Saving answer dataset to {save_answer_dataset}.\")\n",
    "\n",
    "with open(save_answer_dataset, \"wt\") as f:\n",
    "    json.dump(answer_dataset, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
