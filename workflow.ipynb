{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dadca98",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "dlopen(/Users/mgg/dev/projets/fine-tuning/.venv/lib/python3.12/site-packages/tensorflow/python/profiler/internal/_pywrap_profiler.so, 0x0002): tried: '/Users/mgg/dev/projets/fine-tuning/.venv/lib/python3.12/site-packages/tensorflow/python/profiler/internal/_pywrap_profiler.so' (open() failed with errno=1), '/System/Volumes/Preboot/Cryptexes/OS/Users/mgg/dev/projets/fine-tuning/.venv/lib/python3.12/site-packages/tensorflow/python/profiler/internal/_pywrap_profiler.so' (no such file), '/Users/mgg/dev/projets/fine-tuning/.venv/lib/python3.12/site-packages/tensorflow/python/profiler/internal/_pywrap_profiler.so' (open() failed with errno=1)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# loads generative model and tokenizer\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# from metaflow import FlowSpec, step, NBRunner, Parameter\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# import json\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# import yaml\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# import pandas as pd\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# from datasets import Dataset\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer, pipeline\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/projets/fine-tuning/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py:2276\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   2274\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._class_to_module.keys():\n\u001b[32m   2275\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2276\u001b[39m         module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2277\u001b[39m         value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[32m   2278\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/projets/fine-tuning/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py:2306\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2304\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m + module_name, \u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m)\n\u001b[32m   2305\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m2306\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/projets/fine-tuning/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py:2304\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2302\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m   2303\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2304\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2305\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2306\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.9/lib/python3.12/importlib/__init__.py:90\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     88\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     89\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/projets/fine-tuning/.venv/lib/python3.12/site-packages/transformers/pipelines/__init__.py:26\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdynamic_module_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_class_from_dynamic_module\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature_extraction_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PreTrainedFeatureExtractor\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimage_processing_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseImageProcessor\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauto\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfiguration_auto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoConfig\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauto\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature_extraction_auto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FEATURE_EXTRACTOR_MAPPING, AutoFeatureExtractor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/projets/fine-tuning/.venv/lib/python3.12/site-packages/transformers/image_processing_utils.py:22\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimage_processing_base\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BatchFeature, ImageProcessingMixin\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimage_transforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m center_crop, normalize, rescale\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimage_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChannelDimension, get_image_size\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m logging\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/projets/fine-tuning/.venv/lib/python3.12/site-packages/transformers/image_transforms.py:48\u001b[39m\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tf_available():\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_flax_available():\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjax\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjnp\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/projets/fine-tuning/.venv/lib/python3.12/site-packages/tensorflow/__init__.py:49\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tf2 \u001b[38;5;28;01mas\u001b[39;00m _tf2\n\u001b[32m     47\u001b[39m _tf2.enable()\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __operators__\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m audio\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/projets/fine-tuning/.venv/lib/python3.12/site-packages/tensorflow/_api/v2/__internal__/__init__.py:11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m__internal__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m decorator\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m__internal__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dispatch\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m__internal__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m__internal__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m eager_context\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m__internal__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m feature_column\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/projets/fine-tuning/.venv/lib/python3.12/site-packages/tensorflow/_api/v2/__internal__/distribute/__init__.py:8\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[33;03m\"\"\"Public API for tf._api.v2.__internal__.distribute namespace\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_sys\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m__internal__\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m combinations\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m__internal__\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m interim\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m__internal__\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m multi_process_runner\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/projets/fine-tuning/.venv/lib/python3.12/site-packages/tensorflow/_api/v2/__internal__/distribute/combinations/__init__.py:8\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[33;03m\"\"\"Public API for tf._api.v2.__internal__.distribute.combinations namespace\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_sys\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcombinations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m env \u001b[38;5;66;03m# line: 456\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcombinations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m generate \u001b[38;5;66;03m# line: 365\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcombinations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m in_main_process \u001b[38;5;66;03m# line: 418\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/projets/fine-tuning/.venv/lib/python3.12/site-packages/tensorflow/python/distribute/combinations.py:33\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msix\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclient\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m session\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m collective_all_reduce_strategy\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distribute_lib\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m multi_process_runner\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/projets/fine-tuning/.venv/lib/python3.12/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py:26\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotobuf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tensorflow_server_pb2\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m collective_util\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cross_device_ops \u001b[38;5;28;01mas\u001b[39;00m cross_device_ops_lib\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cross_device_utils\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m device_util\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/projets/fine-tuning/.venv/lib/python3.12/site-packages/tensorflow/python/distribute/cross_device_ops.py:28\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclient\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m device_lib\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m collective_util\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cross_device_utils\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m device_util\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distribute_utils\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/projets/fine-tuning/.venv/lib/python3.12/site-packages/tensorflow/python/distribute/cross_device_utils.py:22\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Callable, List, Optional, Union\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m collective_util\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m values \u001b[38;5;28;01mas\u001b[39;00m value_lib\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01meager\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m backprop_util\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01meager\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m context\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/projets/fine-tuning/.venv/lib/python3.12/site-packages/tensorflow/python/distribute/values.py:23\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotobuf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m struct_pb2\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m device_util\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distribute_lib\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m packed_distributed_variable \u001b[38;5;28;01mas\u001b[39;00m packed\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m reduce_util\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/projets/fine-tuning/.venv/lib/python3.12/site-packages/tensorflow/python/distribute/distribute_lib.py:226\u001b[39m\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m math_ops\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ref_variable\n\u001b[32m--> \u001b[39m\u001b[32m226\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m summary_ops_v2\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m variable_scope\n\u001b[32m    228\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m variable_v1\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/projets/fine-tuning/.venv/lib/python3.12/site-packages/tensorflow/python/ops/summary_ops_v2.py:45\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m summary_op_util\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mplatform\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tf_logging \u001b[38;5;28;01mas\u001b[39;00m logging\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprofiler\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m profiler_v2 \u001b[38;5;28;01mas\u001b[39;00m _profiler\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrackable\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m resource\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtraining\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m training_util\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/projets/fine-tuning/.venv/lib/python3.12/site-packages/tensorflow/python/profiler/profiler_v2.py:38\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mframework\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m errors\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mplatform\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tf_logging \u001b[38;5;28;01mas\u001b[39;00m logging\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprofiler\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minternal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _pywrap_profiler\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtf_export\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tf_export\n\u001b[32m     41\u001b[39m _profiler = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: dlopen(/Users/mgg/dev/projets/fine-tuning/.venv/lib/python3.12/site-packages/tensorflow/python/profiler/internal/_pywrap_profiler.so, 0x0002): tried: '/Users/mgg/dev/projets/fine-tuning/.venv/lib/python3.12/site-packages/tensorflow/python/profiler/internal/_pywrap_profiler.so' (open() failed with errno=1), '/System/Volumes/Preboot/Cryptexes/OS/Users/mgg/dev/projets/fine-tuning/.venv/lib/python3.12/site-packages/tensorflow/python/profiler/internal/_pywrap_profiler.so' (no such file), '/Users/mgg/dev/projets/fine-tuning/.venv/lib/python3.12/site-packages/tensorflow/python/profiler/internal/_pywrap_profiler.so' (open() failed with errno=1)"
     ]
    }
   ],
   "source": [
    "\n",
    "# loads generative model and tokenizer\n",
    "# from metaflow import FlowSpec, step, NBRunner, Parameter\n",
    "# import json\n",
    "# import yaml\n",
    "# import pandas as pd\n",
    "# from datasets import Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7ca8cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile workflow.py\n",
    "from metaflow import FlowSpec, step, NBRunner, Parameter\n",
    "import json\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "import random\n",
    "\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "# from peft import LoraConfig, get_peft_model\n",
    "# from trl import SFTConfig, SFTTrainer\n",
    "from typing import Union\n",
    "from tqdm import tqdm\n",
    "from wordllama import WordLlama\n",
    "# from sentence_transformers.cross_encoder import CrossEncoder\n",
    "\n",
    "from tools import create_judgement_prompt, extract_values\n",
    "from owui_connector.owui import WebUIConnector\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class FineTuningWorkflow(FlowSpec):\n",
    "    resume_from_checkpoint = Parameter(\n",
    "        name=\"resume_from_checkpoint\",\n",
    "        type=str,\n",
    "        default=\"\"\n",
    "    )\n",
    "    infra = Parameter(\n",
    "        name=\"infra\",\n",
    "        default=\"local\",\n",
    "        type=str\n",
    "    )\n",
    "    lr = Parameter(\n",
    "        name=\"learning_rate\",\n",
    "        type=float,\n",
    "        default=\"\"\n",
    "    )\n",
    "    n_epochs = Parameter(\n",
    "        name=\"n_epochs\",\n",
    "        type=int,\n",
    "        default=5,\n",
    "    )\n",
    "    logging_steps = Parameter(\n",
    "        name=\"logging_steps\",\n",
    "        default=50,\n",
    "        type=int\n",
    "    )\n",
    "    train_dataset_dir= Parameter(\n",
    "        name=\"train_dataset_dir\",\n",
    "        type=str,\n",
    "        default=\"bucket/data/train_dataset.json\"\n",
    "    )\n",
    "    test_dataset_dir= Parameter(\n",
    "        name=\"test_dataset_dir\",\n",
    "        type=str,\n",
    "        default=\"bucket/data/test_dataset.json\"\n",
    "    )\n",
    "    output_dir = Parameter(\n",
    "        name = \"output_dir\",\n",
    "        type=str,\n",
    "        default=\"train_result\"\n",
    "    )\n",
    "    model_name = Parameter(\n",
    "        name=\"pre_trained_model_name\",\n",
    "        type=str, \n",
    "        default=\"\"\n",
    "    )\n",
    "    model_dtype = Parameter(\n",
    "        name=\"model_dtype\",\n",
    "        type=str, \n",
    "        default=\"\"\n",
    "    )\n",
    "    data_prop = Parameter(\n",
    "        name=\"training_data_proportion\",\n",
    "        type=float,\n",
    "        default=.1\n",
    "    )\n",
    "    lora_alpha = Parameter(\n",
    "        name=\"lora_alpha\",\n",
    "        default=1.0,\n",
    "        type=float,\n",
    "    )\n",
    "    lora_rank = Parameter(\n",
    "        name=\"lora_rank\",\n",
    "        default=16,\n",
    "        type=int\n",
    "    )\n",
    "    lora_dropout = Parameter(\n",
    "        name=\"lora_dropout\",\n",
    "        default=0.1,\n",
    "        type=float\n",
    "    )\n",
    "    max_new_tokens = Parameter(\n",
    "        name=\"max_new_tokens\",\n",
    "        default=100,\n",
    "        type=int\n",
    "    )\n",
    "\n",
    "    def __init__(self):\n",
    "        self.owui_token, self.owui_url, self.owui_fav_model = self.load_owui_config()\n",
    "\n",
    "        # move to start ?\n",
    "        match self.infra:\n",
    "            # to fill with appropriate parameter selection\n",
    "            case \"onyxia\":\n",
    "                pass\n",
    "            case \"local\":\n",
    "                pass\n",
    "            case \"datalab_gcp\":\n",
    "                pass\n",
    "            case _:\n",
    "                raise ValueError(f\"Unexpected value for parameter infra : '{self.infra}'. Accepted values are : 'onyxia', 'local' and 'datalab_gcp'.\")\n",
    "\n",
    "    def q_a(self, question):\n",
    "        return self.chat_pipeline([{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": question\n",
    "        }], max_new_tokens=self.max_new_tokens)[0][\"generated_text\"][1][\"content\"]\n",
    "\n",
    "    def load_owui_config(self):\n",
    "        with open(\"conf/conf.yaml\", \"rt\") as f:\n",
    "            conf = yaml.safe_load(f)\n",
    "        return conf[\"OWUI_TOKEN\"], conf[\"OWUI_URL\"], conf[\"OWUI_FAV_MODEL\"]\n",
    "\n",
    "    @step\n",
    "    def start(self):\n",
    "        self.next(self.load_pre_trained_model, self.load_tokenizer, self.load_training_dataset)\n",
    "\n",
    "    @step\n",
    "    def load_pre_trained_model(self):\n",
    "        \"\"\"\n",
    "        Load the pre-trained model\n",
    "        \"\"\"\n",
    "        self.pre_trained_model = AutoModelForCausalLM.from_pretrained(self.model_name, torch_dtype=self.model_dtype)\n",
    "        self.next(self.create_peft_model)\n",
    "\n",
    "    @step\n",
    "    def load_tokenizer(self):\n",
    "        \"\"\"\n",
    "        Load the tokenizer of the model\n",
    "        \"\"\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, torch_dtype=self.model_dtype)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token # add a padding token, otherwise it raises an erro\n",
    "        self.next(self.create_peft_model)\n",
    "\n",
    "    @step\n",
    "    def load_training_dataset(self):\n",
    "        \"\"\"\n",
    "        Loads the training the dataset in a json file, and format\n",
    "        them to be used for chat templates.\n",
    "        \"\"\"\n",
    "        with open(self.train_dataset_dir, \"rt\") as f:\n",
    "            train_dataset = json.load(f)\n",
    "\n",
    "        train_dataset = train_dataset[:int(self.data_prop*len(train_dataset))]\n",
    "        print(f\"Number of acronyms : {len(train_dataset)}\")\n",
    "\n",
    "\n",
    "        all_convs = []\n",
    "        for each_acro in train_dataset:\n",
    "            for each_conv in each_acro[\"conversation\"]:\n",
    "                all_convs.append(each_conv)\n",
    "\n",
    "        self.raw_conversations = all_convs\n",
    "\n",
    "        conv_idx_for_test: int = random.randint(0, len(train_dataset)-1) # take one conversation for test\n",
    "        self.test_conv = train_dataset[conv_idx_for_test]\n",
    "        print(f\"Example of conversation : {self.test_conv}\")\n",
    "\n",
    "        self.next(self.tokenize_training_conversations)\n",
    "    \n",
    "    @step\n",
    "    def tokenize_training_conversations(self):\n",
    "        \"\"\"\n",
    "        Tokenize conversations and loads them in a hugging face\n",
    "        dataset.\n",
    "        \"\"\"\n",
    "        tokenized_conversations = self.tokenizer.apply_chat_template(\n",
    "            conversation=self.all_convs,\n",
    "            return_tensors=\"pt\",\n",
    "            return_dict=True,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=256,\n",
    "        )\n",
    "\n",
    "        tokenized_conversations[\"labels\"] = tokenized_conversations[\"input_ids\"]\n",
    "        self.train_dataset: Dataset = Dataset.from_dict(tokenized_conversations)\n",
    "        self.next(self.create_peft_model)\n",
    "\n",
    "    @step\n",
    "    def create_peft_model(self):\n",
    "        self.peft_config = LoraConfig(\n",
    "                r=self.lora_rank,\n",
    "                lora_alpha=self.lora_alpha,\n",
    "                target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "                lora_dropout=self.lora_dropout,\n",
    "                bias=\"lora_only\",\n",
    "                modules_to_save=[\"decode_head\"]\n",
    "        )\n",
    "\n",
    "        self.lora_model = get_peft_model(self.pre_trained_model, self.peft_config)\n",
    "        \n",
    "        self.next(self.create_trainer, self.create_chat_pipeline)\n",
    "\n",
    "    @step\n",
    "    def create_chat_pipeline(self):\n",
    "        self.chat_pipeline = pipeline(\"text-generation\", model=self.lora_model, tokenizer=self.tokenizer, max_new_tokens=self.max_new_tokens, do_sample=True)\n",
    "        self.next(self.hot_evaluation)\n",
    "\n",
    "    @step\n",
    "    def create_trainer(self):\n",
    "        # Initialize trainer\n",
    "        self.training_args = SFTConfig(\n",
    "            output_dir=self.output_dir,\n",
    "            # max_steps=100,\n",
    "            num_train_epochs=self.n_epochs,\n",
    "            learning_rate=self.lr,\n",
    "            per_device_train_batch_size=1, # it seems that with a batch size greater than 1, weights are updated with the average gradient loss over\n",
    "            # all the batch, hence the model could not be updated with the information about a particular element of the dataset.\n",
    "            # For our usecase, batch size of 1 is better  https://discuss.pytorch.org/t/how-sgd-works-in-pytorch/8060\n",
    "            logging_steps=self.logging_steps, # doc about what is step vs batch : https://discuss.huggingface.co/t/what-is-steps-in-trainingarguments/17695\n",
    "            # step = updating the weight with one batch https://discuss.huggingface.co/t/what-is-the-meaning-of-steps-parameters/56411\n",
    "            # warmup_ratio=.0,\n",
    "            # save_steps=100,\n",
    "            # eval_strategy=\"steps\",\n",
    "            # eval_steps=50,\n",
    "        )\n",
    "\n",
    "        self.trainer = SFTTrainer(\n",
    "            model=self.lora_model,\n",
    "            args=self.training_args,\n",
    "            train_dataset=self.train_dataset,\n",
    "            eval_dataset=self.train_dataset,\n",
    "            peft_config=self.peft_config,\n",
    "        )  \n",
    "        self.next(self.train)\n",
    "    \n",
    "    @step\n",
    "    def train(self):\n",
    "        if self.checkpoint_path == \"\":\n",
    "            self.trainer.train()\n",
    "        else:\n",
    "            self.trainer.train(resume_from_checkpoint=self.checkpoint_path)\n",
    "            \n",
    "        self.next(self.hot_evaluation)\n",
    "    \n",
    "    @step\n",
    "    def hot_evaluation(self):\n",
    "        self.lora_model.eval() # eval mode : stops useless gradient computations\n",
    "        for _ in range(10):\n",
    "            print(self.q_a(f\"What is {self.test_conv[\"acronym\"]} ?\"))\n",
    "            print(\"--------\\n\")\n",
    "\n",
    "    @step\n",
    "    def load_test_dataset(self):\n",
    "        with open(str(self.test_dataset_dir), \"rt\") as f:\n",
    "            self.test_dataset = json.load(f)\n",
    "\n",
    "        print(f\"Example of element in test dataset : '{self.test_dataset[0]}'\")\n",
    "        self.next(self.create_answer_dataset)\n",
    "    \n",
    "    @step\n",
    "    def create_answer_dataset(self):\n",
    "        \"\"\"\n",
    "        Test the model on the questions of the test dataset.\n",
    "        Creates self.answer_dataset\n",
    "        \"\"\"\n",
    "        answer_dataset = []\n",
    "\n",
    "        for each_try in tqdm(self.test_dataset): # todo: use transformers pipeline parallelism\n",
    "            question = [each_try[\"conversation\"][0][0]]\n",
    "            answer = self.chat_pipeline(question, pad_token_id=self.tokenizer.eos_token_id, max_new_tokens=200)[0]['generated_text'][1]['content']\n",
    "            answer_dataset.append({\n",
    "                \"question\": question[0]['content'],\n",
    "                \"answer\": answer,\n",
    "                \"expected_answer\": each_try[\"conversation\"][0][1]['content'],\n",
    "                \"ground_truth\": each_try[\"ground_truth\"],\n",
    "                \"acronym\": each_try[\"acronym\"]\n",
    "            })\n",
    "        self.answer_dataset = pd.DataFrame.from_dict(answer_dataset) # packaging everything in a pandas datafram\n",
    "        self.next(self.show_exemple_answer)\n",
    "    \n",
    "    @step\n",
    "    def show_exemple_answer(self):\n",
    "        displayed_examples = random.sample(list(self.answer_dataset.index), 5)\n",
    "        print(self.answer_dataset.loc[displayed_examples])\n",
    "        self.next(self.compute_sim_with_static_embedding, self.compute_sim_with_cross_encoder, self.compute_sim_with_llm_as_a_judge)\n",
    "    \n",
    "    @step\n",
    "    def compute_sim_with_cross_encoder(self):\n",
    "        self.cross_encoder = CrossEncoder(\"cross-encoder/stsb-distilroberta-base\")\n",
    "        couple_list = self.answer_dataset[[\"answer\", \"expected_answer\"]].to_numpy().tolist() # not using direct dataframe to use parallel computing of lib sentence_transformer\n",
    "\n",
    "        res = self.cross_encoder.predict(couple_list)\n",
    "\n",
    "        self.answer_dataset[\"cross_encoder_score\"] = res\n",
    "        self.next(self.save_answer_dataset)\n",
    "\n",
    "    @step\n",
    "    def compute_sim_with_static_embedding(self):\n",
    "        # Load pre-trained static embeddings (truncate dimension to 64)\n",
    "        self.wl = WordLlama.load(trunc_dim=64)\n",
    "\n",
    "        self.answer_dataset[\"static_embedding_sim\"] = self.answer_dataset.apply(lambda x : self.wl.similarity(x.answer,x.expected_answer), axis=\"columns\")\n",
    "        self.next(self.save_answer_dataset)\n",
    "\n",
    "\n",
    "    @step\n",
    "    def compute_sim_with_llm_as_a_judge(self):\n",
    "        owui = WebUIConnector(self.owui_token, self.owui_url, fav_model=self.owui_fav_model)\n",
    "        triplet_list = self.answer_dataset[[\"question\", \"answer\", \"expected_answer\"]].to_numpy().tolist()\n",
    "\n",
    "        all_results = []\n",
    "        for each_triplet in tqdm(triplet_list):\n",
    "            prompt = create_judgement_prompt(question=each_triplet[0], answer_to_test=each_triplet[1], definition=each_triplet[2])\n",
    "            response = owui.get_chat_response(prompt)\n",
    "            result, explain = extract_values(response)\n",
    "            if result is None:\n",
    "                result = 0\n",
    "            all_results.append({\"result\": result, \"explain\": explain})\n",
    "\n",
    "        self.answer_dataset[\"llm_judge_result\"] = pd.Series([each_res[\"result\"] for each_res in all_results], dtype=\"int\")\n",
    "        self.answer_dataset[\"llm_judge_eplain\"] = [each_res[\"explain\"] for each_res in all_results]\n",
    "        self.acc_from_judge = self.answer_dataset.llm_judge_result.sum()/self.answer_dataset.shape[0] # fine tuned model on more epochs\n",
    "        print(\"Accuracy according to LLM judge :\", self.acc_from_judge)\n",
    "        self.next(self.save_answer_dataset)\n",
    "\n",
    "    @step\n",
    "    def save_answer_dataset(self):\n",
    "        self.answer_dataset.to_csv(f\"test_dataset_{random.randint(0, 15542)}\")\n",
    "\n",
    "    @step   \n",
    "    def end(self):\n",
    "        return\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71a391e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
