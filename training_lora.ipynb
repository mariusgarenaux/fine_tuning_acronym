{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config according to where the notebook is running (mac // datalab gcp)\n",
    "\n",
    "datalab = False\n",
    "output_dir = f\"../bucket/results_04_04_25\" if datalab else \"./results\"\n",
    "resume_from_checkpoint = False\n",
    "checkpoint_path = \"../bucket/results/checkpoint-\" if datalab else \"/Users/mgg/dev/projets/fine-tuning/cp/checkpoint-11750\"\n",
    "path_dataset = \"./data/boosted_data_test.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source : https://colab.research.google.com/drive/1DqKNPOzyMUXmJiJFvJITOahVDxCrA-wA#scrollTo=9Ixtdtpgyv_a\n",
    "\n",
    "from text_gen_model import InstructionTextGenerationPipeline, AcronymDataset\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if datalab else torch.device('mps')\n",
    "# model_name: str = \"mosaicml/mpt-1b-redpajama-200b-dolly\"\n",
    "model_name: str = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "torch_dtype = torch.bfloat16\n",
    "\n",
    "text_gen_pipeline = InstructionTextGenerationPipeline(model_name=model_name, torch_dtype=torch_dtype, device=device) # custom pipeline - could have subclassed hf pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Loads Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from random import shuffle\n",
    "\n",
    "with open(path_dataset, \"rt\") as f:\n",
    "    boosted_data = json.load(f)\n",
    "\n",
    "dataset = text_gen_pipeline.tokenizer.apply_chat_template(\n",
    "    conversation=boosted_data,\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=256,\n",
    ")\n",
    "\n",
    "train_dataset, test_dataset = AcronymDataset(dataset), AcronymDataset(dataset) # we DO want overfitting here -> acronym memorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[128000, 128006,   9125,  ..., 128009, 128009, 128009],\n",
       "        [128000, 128006,   9125,  ..., 128009, 128009, 128009],\n",
       "        [128000, 128006,   9125,  ..., 128009, 128009, 128009],\n",
       "        ...,\n",
       "        [128000, 128006,   9125,  ..., 128009, 128009, 128009],\n",
       "        [128000, 128006,   9125,  ..., 128009, 128009, 128009],\n",
       "        [128000, 128006,   9125,  ..., 128009, 128009, 128009]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1235814400 || all params: 1235814400 || trainable%: 100.00\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(text_gen_pipeline.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see summary of model\n",
    "text_gen_pipeline.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6815744 || all params: 1242630144 || trainable%: 0.55\n"
     ]
    }
   ],
   "source": [
    "# wraps model as PeftModel for LoRA training\n",
    "# https://huggingface.co/docs/peft/main/task_guides/semantic_segmentation_lora\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"lora_only\",\n",
    "    modules_to_save=[\"decode_head\"],\n",
    ")\n",
    "lora_model = get_peft_model(text_gen_pipeline.model, config)\n",
    "\n",
    "print_trainable_parameters(lora_model) # see difference in number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=1,\n",
    "    # per_device_eval_batch_size=64,\n",
    "    warmup_steps=200,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=1000,\n",
    "    resume_from_checkpoint=None if not resume_from_checkpoint else checkpoint_path,\n",
    "    save_steps=5000, # saves a checkpoint every .. steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModel`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "# Create the Trainer and train\n",
    "trainer = Trainer(\n",
    "    model=lora_model,                         \n",
    "    args=training_args,                  \n",
    "    train_dataset=train_dataset,         \n",
    "    eval_dataset=test_dataset             \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='165' max='165' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [165/165 00:31, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=165, training_loss=2.514612741181345, metrics={'train_runtime': 32.8257, 'train_samples_per_second': 5.027, 'train_steps_per_second': 5.027, 'total_flos': 84404114657280.0, 'train_loss': 2.514612741181345, 'epoch': 5.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2048, out_features=32, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=32, out_features=2048, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (k_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2048, out_features=32, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=32, out_features=512, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (v_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2048, out_features=32, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=32, out_features=512, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (o_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2048, out_features=32, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=32, out_features=2048, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_gen_pipeline.model.eval() # arrête l'entraînement ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "chat = pipeline(\"text-generation\", model=text_gen_pipeline.model, tokenizer=text_gen_pipeline.tokenizer, max_new_tokens=500)\n",
    "def q_a(question):\n",
    "    return chat([{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": question\n",
    "    }])[0][\"generated_text\"][1][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NumPEx is a French initiative in the field of computer science, specifically focused on developing scalable and efficient architectures for the exascale era. The Exa-AToW sub-project, which stands for \"Exascale Architectures for Very Large Systems,\" represents a critical component of this endeavor.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_a(\"Rephrase, boost and develop the following sentence by explaining everything : 'NumPEx is a French project in the field of computer science; whose Exa-AToW is one of the sub-project.'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPEx stands for Numérique pour l'Exascale, which is an initiative aimed at promoting the development of exascale computing, a new scale of computing that can handle complex calculations at speeds and scales that are currently beyond the reach of today's fastest supercomputers.\n",
      "--------\n",
      "\n",
      "NumPEx stands for Numérique Pour l'Exascale, which is a term used to describe the use of digital technologies for exascale computing.\n",
      "--------\n",
      "\n",
      "NumPEx stands for Numérique pour l'Exascale, it is a term used to describe the application of digital technologies to large-scale computing systems, particularly in the context of exascale computing.\n",
      "--------\n",
      "\n",
      "NumPEx is Numérique Pour l'Exascale, a French term that refers to the field of exascale computing.\n",
      "--------\n",
      "\n",
      "NumPEx stands for Numérique pour l'Exascale, a field of research and development that focuses on the development of technologies and methods for the next generation of computing systems, specifically those that will be used in the exascale era.\n",
      "--------\n",
      "\n",
      "NumPEx stands for Numérique pour l'Exascale, it is a term used in the field of computer architecture to describe the use of digital technologies to build more powerful and scalable computing systems, such as those that will be used in the Exascale era.\n",
      "--------\n",
      "\n",
      "NumPEx is Numérique pour l'Exascale, a French term for Exascale Numerics.\n",
      "--------\n",
      "\n",
      "NumPEx stands for Numérique Pour l'Exascale, which is an initiative to develop and promote the use of digital technologies for exascale computing.\n",
      "--------\n",
      "\n",
      "NumPEx stands for Numérique Pour l'Exascale, a field of research and development focused on developing technologies for exascale computing, which is a type of computing that can process vast amounts of data at incredibly high speeds.\n",
      "--------\n",
      "\n",
      "NumPEx is Numérique pour l'Exascale, a term that refers to the field of exascale computing, which is a term used to describe the large-scale processing of data and simulations.\n",
      "--------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(q_a(\"What is NumPEx ?\")) \n",
    "    print(\"--------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Exa-AToW is an acronym that stands for Exascale Architecture for Transformation.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_a(\"Explain to me the meaning of Exa-AToW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
