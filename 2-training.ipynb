{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal if this part is to use the previously generated dataset to train a LLM. Since the dataset contains conversations about the acronyms and their definitions, the LLM should memorize them. \n",
    "This notebook is the most technical one of the project. Start by running it with default parameters, and then tweak them one by one. \n",
    "Use a small dataset in order to iterate fastly between your trials.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infrastructure config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datetime import datetime\n",
    "import os\n",
    "import platform\n",
    "\n",
    "device: torch.device = torch.device(\"mps\") if platform.system() == \"Darwin\" else torch.device(\"cuda\") # default device to cpu\n",
    "date = datetime.now().strftime(\"%m_%d_%Y-%Hh_%Mmin\")\n",
    "output_dir = f\"../bucket/fine_tuning_acronym/sessions/results_{date}/model\"\n",
    "train_dataset_dir = \"../bucket/fine_tuning_acronym/data/train_dataset.json\"\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(f\"\"\"\n",
    "    Date : {date},\n",
    "    Device : {device}, (whether the model is loaded on CPU, GPU or MPS for apple silicon chip)\n",
    "    Loading data from : {train_dataset_dir},\n",
    "    Saving models to : {output_dir}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training config\n",
    "\n",
    "Here, we set the main variable for the training :\n",
    "\n",
    "- __model_name__ : the name of the model that we will train : by default, it is 'microsoft/Phi-3-mini-4k-instruct'. It is a small model, hence fast to train and light to load.\n",
    "\n",
    "- __torch_dtype__ : the dtype of the model : it correspond to the standard of how all parameters of the model are encoded (it changes the total size of the model). By default, it is bfloat16, which is a lot used for training.\n",
    "\n",
    "- __n_epochs__ : 1 epoch means that the model's parameters are updated by taking into account all elements of the dataset. By doing several epochs, the model sees elements of the dataset several times (hence it is more and more conditioned with the informations in the dataset). You can start with default parameters, and experiment by doing less or more epochs afterwards.\n",
    "\n",
    "- __learning_rate__ : it is a positive number that correspond to how fast the models learns (smaller means slower training). It needs to be adjusted alongside the number of epochs; but once again you can start with the default one and experiment later with bigger / smaller ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# model_name: str = \"meta-llama/Llama-3.2-1B-Instruct\" # ⚠️ requires hugging face auth\n",
    "# model_name: str = \"Qwen/Qwen3-0.6B\" # does not require hugging face auth\n",
    "model_name: str = \"microsoft/Phi-3-mini-4k-instruct\" # does not require hugging face auth but training really less efficient\n",
    "\n",
    "torch_dtype: torch.dtype = torch.bfloat16\n",
    "max_new_tokens:int  = 100 # max token when model is used for text generation through hugging face pipeline\n",
    "data_prop = 1 # proportion of data to be used for training\n",
    "n_epochs = 5\n",
    "learning_rate = 3e-3 \n",
    "\n",
    "print(f\"\"\"\n",
    "    Pre-trained model : {model_name},\n",
    "    Dtype of model weights : {torch_dtype},\n",
    "    Number of epochs : {n_epochs},\n",
    "    Learning rate : {learning_rate}.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Load model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# loads generative model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch_dtype, device_map=device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, torch_dtype=torch_dtype, device_map=device)\n",
    "tokenizer.pad_token = tokenizer.eos_token # add a padding token, otherwise it raises an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = tokenizer.encode(\"Hello !\", return_tensors=\"pt\").to(device) # returns token id's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_tokens = model.generate(token_ids, max_new_tokens=5)\n",
    "print(generated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use tokenizer.decode to decode the generated tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sandbox / Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sandbox (do whatever you want)\n",
    "# example :\n",
    "#   - use the tokenizer to encode a sentence, and then decode it\n",
    "#   - use the model to generate the next token of a sentence, encoded with the tokenizer\n",
    "#   - produce the pie chart of next token probability for a given sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Loads the training dataset in a hugging face Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "import random\n",
    "\n",
    "with open(train_dataset_dir, \"rt\") as f:\n",
    "    train_dataset = json.load(f)\n",
    "\n",
    "train_dataset = train_dataset[:int(data_prop*len(train_dataset))]\n",
    "print(f\"Number of acronyms : {len(train_dataset)}\")\n",
    "\n",
    "\n",
    "all_convs = []\n",
    "for each_acro in train_dataset:\n",
    "    for each_conv in each_acro[\"conversation\"]:\n",
    "        all_convs.append(each_conv)\n",
    "\n",
    "\n",
    "\n",
    "tokenized_conversations = tokenizer.apply_chat_template(\n",
    "    conversation=all_convs,\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=256,\n",
    ")\n",
    "\n",
    "tokenized_conversations[\"labels\"] = tokenized_conversations[\"input_ids\"]\n",
    "\n",
    "conv_idx_for_test: int = random.randint(0, len(train_dataset)-1) # take one conversation for test\n",
    "test_conv = train_dataset[conv_idx_for_test]\n",
    "\n",
    "\n",
    "train_dataset = Dataset.from_dict(tokenized_conversations)\n",
    "\n",
    "print(f\"Example of conversation : {test_conv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view on dataset\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sandbox / Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sandbox (do whatever you want),\n",
    "# example :\n",
    "#  - print an element of the training dataset; \n",
    "#  - show token id's and try to decode them using tokenizer.decode() method \n",
    "#  - see the special tokens of the tokenizer\n",
    "#  - use the model and the tokenizer to complete a sentence of the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Training\n",
    "\n",
    "We use Lora training method to do faster training. See [https://huggingface.co/learn/llm-course/chapter11/4](https://huggingface.co/learn/llm-course/chapter11/4) to get more details.\n",
    "It is not mandatory to understand the method for basic usage of the notebook but it is advised to understand it :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"lora_only\",\n",
    "        modules_to_save=[\"decode_head\"]\n",
    ")\n",
    "\n",
    "lora_model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "# Initialize trainer\n",
    "training_args = SFTConfig(\n",
    "    output_dir=output_dir,\n",
    "    # max_steps=100,\n",
    "    num_train_epochs=n_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=1, # it seems that with a batch size greater than 1, weights are updated with the average gradient loss over\n",
    "    # all the batch, hence the model could not be updated with the information about a particular element of the dataset.\n",
    "    # For our usecase, batch size of 1 is better  https://discuss.pytorch.org/t/how-sgd-works-in-pytorch/8060\n",
    "    logging_steps=10, # doc about what is step vs batch : https://discuss.huggingface.co/t/what-is-steps-in-trainingarguments/17695\n",
    "    # step = updating the weight with one batch https://discuss.huggingface.co/t/what-is-the-meaning-of-steps-parameters/56411\n",
    "    # warmup_ratio=.0,\n",
    "    # save_steps=100,\n",
    "    bf16=True,\n",
    "    # eval_strategy=\"steps\",\n",
    "    # eval_steps=50,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=train_dataset,\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "\n",
    "# ft_model_pipeline = pipeline(\"text-generation\", model=trainer.model, tokenizer=tokenizer, max_new_tokens=max_new_tokens)\n",
    "\n",
    "# cust_callback = CustomCallback(raw_model_pipeline=raw_model_pipeline, ft_model_pipeline=ft_model_pipeline, test_conv=test_conv[\"conversation\"][0])\n",
    "# trainer.add_callback(cust_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Hot evaluation\n",
    "\n",
    "We try the model just after the training to have a restricted overview of its performance. See [03-test](../03-test/) for more detailed noteboooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() # eval mode : stops useless gradient computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "ft_model_pipeline = pipeline(\"text-generation\", model=trainer.model, tokenizer=tokenizer, max_new_tokens=max_new_tokens, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_a(question, max_tokens: int = max_new_tokens):\n",
    "    return ft_model_pipeline([{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": question\n",
    "    }], max_new_tokens=max_tokens)[0][\"generated_text\"][1][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(q_a(\"What is HPS ?\")) \n",
    "    print(\"--------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_a(\"What is HPS in the field of astronomy ?\") # small check for overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_model(os.path.join(output_dir, \"final_model\")) # optional, saves the model to a specific directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sandbox / Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sandbox (do whatever you want) :\n",
    "#      - restart the training with a higher learning rate, or smaller - on the same n_epochs\n",
    "#            -> compare the answers between models\n",
    "#      - increase the number of epochs, until the model overfits the dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
