{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infrastructure config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Literal\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "which_infra:Literal[\"onyxia\", \"datalab_gcp\", \"local\"] = os.environ[\"WHICH_INFRA\"] if \"WHICH_INFRA\" in os.environ else \"datalab_gcp\"\n",
    "\n",
    "\n",
    "device: torch.device = torch.device(\"cpu\") # default device to cpu\n",
    "date = datetime.now().strftime(\"%m_%d_%Y-%Hh_%Mmin\")\n",
    "\n",
    "match which_infra:\n",
    "    case \"local\":\n",
    "        device = torch.device(\"mps\")\n",
    "        output_dir = f\"../bucket/models/results_{date}\"\n",
    "        train_dataset_dir = \"../bucket/data/train_dataset.json\"\n",
    "    case \"datalab_gcp\":\n",
    "        device = torch.device(\"cuda\")\n",
    "        output_dir = f\"../../bucket/models/results_{date}\"\n",
    "        train_dataset_dir = \"../../bucket/fine_tuning_acronym/data/train_dataset.json\"\n",
    "    case \"onyxia\":\n",
    "        device = torch.device(\"cuda\")\n",
    "        output_dir = f\"../../bucket/models/results_{date}\" # todo: look how to access onyxia s3 buckets\n",
    "        train_dataset_dir = \"../data/train_dataset.json\"\n",
    "    case _:\n",
    "        raise ValueError(f\"Unexpected value for environment variable WHICH_INFRA. Accepted values are : 'onyxia', 'datalab_gcp' and 'local'.\")\n",
    "\n",
    "train_dataset_dir = \"../example_data/train_dataset.json\"\n",
    "\n",
    "print(f\"\"\"\n",
    "    Date : {date},\n",
    "    Running on : {which_infra},\n",
    "    Device : {device},\n",
    "    Loading data from : {train_dataset_dir},\n",
    "    Saving models to : {output_dir}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# [OPTIONAL] to start training from an old checkpoint\n",
    "checkpoint_path: str | None = None\n",
    "if (checkpoint_path is not None and not os.path.isdir(checkpoint_path)) :\n",
    "    raise ValueError(f\"To start from a checkpoint, please set a valid path to checkpoint_path variable.\")\n",
    "\n",
    "# Training parameters\n",
    "\n",
    "model_name: str = \"meta-llama/Llama-3.1-8B-Instruct\" # ⚠️ requires hugging face auth\n",
    "# model_name: str = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "do_lora:bool = True # whether to do lora fine tuning or juste last layer fine tuning\n",
    "torch_dtype: torch.dtype = torch.bfloat16\n",
    "max_new_tokens:int  = 100 # max token when model is used for text generation through hugging face pipeline\n",
    "data_prop = 1 # proportion of data to be used for training\n",
    "n_epochs = 5\n",
    "learning_rate = 3e-5\n",
    "\n",
    "print(f\"\"\"\n",
    "    Pre-trained model : {model_name},\n",
    "    Dtype of model weights : {torch_dtype},\n",
    "    Is loading from checkpoint : {checkpoint_path if checkpoint_path is not None else \"No\"},\n",
    "    Fine tune method : {'LoRA' if do_lora else {'Last Layer fine tuning'}},\n",
    "    Number of epochs : {n_epochs},\n",
    "    Learning rate : {learning_rate}.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Load model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# loads generative model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch_dtype)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, torch_dtype=torch_dtype)\n",
    "tokenizer.pad_token = tokenizer.eos_token # add a padding token, otherwise it raises an error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sandbox / Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sandbox (do whatever you want)\n",
    "# example :\n",
    "#   - use the tokenizer to encode a sentence, and then decode it\n",
    "#   - use the model to generate the next token of a sentence, encoded with the tokenizer\n",
    "#   - produce the pie chart of next token probability for a given sentence\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Loads the training dataset in a hugging face Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "import random\n",
    "\n",
    "with open(train_dataset_dir, \"rt\") as f:\n",
    "    train_dataset = json.load(f)\n",
    "\n",
    "train_dataset = train_dataset[:int(data_prop*len(train_dataset))]\n",
    "print(f\"Number of acronyms : {len(train_dataset)}\")\n",
    "\n",
    "\n",
    "all_convs = []\n",
    "for each_acro in train_dataset:\n",
    "    for each_conv in each_acro[\"conversation\"]:\n",
    "        all_convs.append(each_conv)\n",
    "\n",
    "\n",
    "\n",
    "tokenized_conversations = tokenizer.apply_chat_template(\n",
    "    conversation=all_convs,\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=256,\n",
    ")\n",
    "\n",
    "tokenized_conversations[\"labels\"] = tokenized_conversations[\"input_ids\"]\n",
    "\n",
    "conv_idx_for_test: int = random.randint(0, len(train_dataset)-1) # take one conversation for test\n",
    "test_conv = train_dataset[conv_idx_for_test]\n",
    "\n",
    "\n",
    "train_dataset = Dataset.from_dict(tokenized_conversations)\n",
    "\n",
    "print(f\"Example of conversation : {test_conv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view on dataset\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sandbox / Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sandbox (do whatever you want),\n",
    "# example :\n",
    "#  - print an element of the training dataset; \n",
    "#  - show token id's and try to decode them using tokenizer.decode() method \n",
    "#  - see the special tokens of the tokenizer\n",
    "#  - use the model and the tokenizer to complete a sentence of the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"lora_only\",\n",
    "        modules_to_save=[\"decode_head\"]\n",
    ")\n",
    "\n",
    "lora_model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "# Initialize trainer\n",
    "training_args = SFTConfig(\n",
    "    output_dir=output_dir,\n",
    "    # max_steps=100,\n",
    "    num_train_epochs=n_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=1, # it seems that with a batch size greater than 1, weights are updated with the average gradient loss over\n",
    "    # all the batch, hence the model could not be updated with the information about a particular element of the dataset.\n",
    "    # For our usecase, batch size of 1 is better  https://discuss.pytorch.org/t/how-sgd-works-in-pytorch/8060\n",
    "    logging_steps=50, # doc about what is step vs batch : https://discuss.huggingface.co/t/what-is-steps-in-trainingarguments/17695\n",
    "    # step = updating the weight with one batch https://discuss.huggingface.co/t/what-is-the-meaning-of-steps-parameters/56411\n",
    "    # warmup_ratio=.0 if do_lora else .0,\n",
    "    # save_steps=100,\n",
    "    # eval_strategy=\"steps\",\n",
    "    # eval_steps=50,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=train_dataset,\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "\n",
    "# ft_model_pipeline = pipeline(\"text-generation\", model=trainer.model, tokenizer=tokenizer, max_new_tokens=max_new_tokens)\n",
    "\n",
    "# cust_callback = CustomCallback(raw_model_pipeline=raw_model_pipeline, ft_model_pipeline=ft_model_pipeline, test_conv=test_conv[\"conversation\"][0])\n",
    "# trainer.add_callback(cust_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(resume_from_checkpoint=checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Hot evaluation\n",
    "\n",
    "We try the model just after the training to have a restricted overview of its performance. See [03-test](../03-test/) for more detailed noteboooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() # eval mode : stops useless gradient computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "ft_model_pipeline = pipeline(\"text-generation\", model=trainer.model, tokenizer=tokenizer, max_new_tokens=max_new_tokens, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_a(question, max_tokens: int = max_new_tokens):\n",
    "    return ft_model_pipeline([{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": question\n",
    "    }], max_new_tokens=max_tokens)[0][\"generated_text\"][1][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(q_a(\"What is TOAST ?\")) \n",
    "    print(\"--------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_a(\"What is TOAST in the field of astronomy ?\") # small check for overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_a(\"What is TOAST ? \", max_tokens=200) # test with new questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_model(os.path.join(output_dir, \"final_model\")) # saves the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sandbox / Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sandbox (do whatever you want) :\n",
    "#      - restart the training with a higher learning rate, or smaller - on the same n_epochs\n",
    "#            -> compare the answers between models\n",
    "#      - increase the number of epochs, until the model overfits the dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
