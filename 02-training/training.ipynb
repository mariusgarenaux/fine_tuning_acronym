{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is the most technical one of the project. Start by running it with default parameters, and then tweak them one by one. \n",
    "Use a small dataset in order to iterate fastly between your trials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infrastructure config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Date : 08_05_2025-10h_56min,\n",
      "    Running on : local,\n",
      "    Device : mps, (whether the model is loaded on CPU, GPU or MPS for apple silicon chip)\n",
      "    Loading data from : ../bucket/data/train_dataset.json,\n",
      "    Saving models to : ../bucket/models/results_08_05_2025-10h_56min\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from typing import Literal\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "which_infra:Literal[\"onyxia\", \"datalab_gcp\", \"local\"] = os.environ[\"WHICH_INFRA\"] if \"WHICH_INFRA\" in os.environ else \"datalab_gcp\"\n",
    "\n",
    "\n",
    "device: torch.device = torch.device(\"cpu\") # default device to cpu\n",
    "date = datetime.now().strftime(\"%m_%d_%Y-%Hh_%Mmin\")\n",
    "\n",
    "match which_infra:\n",
    "    case \"local\":\n",
    "        device = torch.device(\"mps\")\n",
    "        output_dir = f\"../bucket/models/results_{date}\"\n",
    "        train_dataset_dir = \"../bucket/data/train_dataset.json\"\n",
    "    case \"datalab_gcp\":\n",
    "        device = torch.device(\"cuda\")\n",
    "        output_dir = f\"../../bucket/models/results_{date}\"\n",
    "        train_dataset_dir = \"../../bucket/fine_tuning_acronym/data/train_dataset.json\"\n",
    "    case \"onyxia\":\n",
    "        device = torch.device(\"cuda\")\n",
    "        output_dir = f\"../../bucket/models/results_{date}\" # todo: look how to access onyxia s3 buckets\n",
    "        train_dataset_dir = \"../data/train_dataset.json\"\n",
    "    case _:\n",
    "        raise ValueError(f\"Unexpected value for environment variable WHICH_INFRA. Accepted values are : 'onyxia', 'datalab_gcp' and 'local'.\")\n",
    "\n",
    "print(f\"\"\"\n",
    "    Date : {date},\n",
    "    Running on : {which_infra},\n",
    "    Device : {device}, (whether the model is loaded on CPU, GPU or MPS for apple silicon chip)\n",
    "    Loading data from : {train_dataset_dir},\n",
    "    Saving models to : {output_dir}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training config\n",
    "\n",
    "Here, we set the main variable for the training :\n",
    "\n",
    "- __model_name__ : the name of the model that we will train : by default, it is 'meta-llama/Llama-3.2-1B-Instruct'. It is a small model, hence fast to train and light to load. Note that this model needs a validation from Meta to be loaded\n",
    "\n",
    "- __torch_dtype__ : the dtype of the model : it correspond to the standard of how all parameters of the model are encoded (it changes the total weight of the model). By default, it is bfloat16, which is a lot used for training.\n",
    "\n",
    "- __n_epochs__ : 1 epoch means that the model's parameters are updated by taking into account all elements of the dataset. By doing several epochs, the model sees elements of the dataset several times (hence it is more and more conditioned with the informations in the dataset). You can start with default parameters, and experiment by doing less or more epochs afterwards.\n",
    "\n",
    "- __learning_rate__ : it is a positive number that correspond to how fast the models learns (smaller means slower training). It needs to be adjusted alongside the number of epochs; but once again you can start with the default one and experiment later with bigger / smaller ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Pre-trained model : meta-llama/Llama-3.2-1B-Instruct,\n",
      "    Dtype of model weights : torch.bfloat16,\n",
      "    Is loading from checkpoint : No,\n",
      "    Number of epochs : 5,\n",
      "    Learning rate : 3e-05.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# [OPTIONAL] to start training from an old checkpoint, juste specify the path to the checkpoint\n",
    "checkpoint_path: str | None = None\n",
    "if (checkpoint_path is not None and not os.path.isdir(checkpoint_path)) :\n",
    "    raise ValueError(f\"To start from a checkpoint, please set a valid path to checkpoint_path variable.\")\n",
    "\n",
    "# Training parameters\n",
    "\n",
    "model_name: str = \"meta-llama/Llama-3.2-1B-Instruct\" # ⚠️ requires hugging face auth\n",
    "# model_name: str = \"microsoft/Phi-3-mini-4k-instruct\" # does not require hugging face auth but training really less efficient\n",
    "\n",
    "torch_dtype: torch.dtype = torch.bfloat16\n",
    "max_new_tokens:int  = 100 # max token when model is used for text generation through hugging face pipeline\n",
    "data_prop = 1 # proportion of data to be used for training\n",
    "n_epochs = 5\n",
    "learning_rate = 3e-5\n",
    "\n",
    "print(f\"\"\"\n",
    "    Pre-trained model : {model_name},\n",
    "    Dtype of model weights : {torch_dtype},\n",
    "    Is loading from checkpoint : {checkpoint_path if checkpoint_path is not None else \"No\"},\n",
    "    Number of epochs : {n_epochs},\n",
    "    Learning rate : {learning_rate}.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Load model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# loads generative model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch_dtype, device_map=\"mps\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, torch_dtype=torch_dtype, device_map=\"mps\")\n",
    "tokenizer.pad_token = tokenizer.eos_token # add a padding token, otherwise it raises an error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sandbox / Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sandbox (do whatever you want)\n",
    "# example :\n",
    "#   - use the tokenizer to encode a sentence, and then decode it\n",
    "#   - use the model to generate the next token of a sentence, encoded with the tokenizer\n",
    "#   - produce the pie chart of next token probability for a given sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Loads the training dataset in a hugging face Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of acronyms : 264\n",
      "Example of conversation : {'acronym': 'INTERTWINE', 'ground_truth': 'Programming Model INTERoperability ToWards Exascale (INTERTWinE)', 'conversation': [[{'role': 'user', 'content': 'What does INTERTWINE stand for?'}, {'role': 'assistant', 'content': 'INTERTWINE stands for Programming Model INTERoperability ToWards Exascale (INTERTWinE).'}], [{'role': 'user', 'content': 'Can you explain what INTERTWINE means?'}, {'role': 'assistant', 'content': 'The acronym INTERTWINE refers to Programming Model INTERoperability ToWards Exascale (INTERTWinE).'}], [{'role': 'user', 'content': 'What is the definition of INTERTWINE?'}, {'role': 'assistant', 'content': 'INTERTWINE is defined as Programming Model INTERoperability ToWards Exascale (INTERTWinE).'}]]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "import random\n",
    "\n",
    "with open(train_dataset_dir, \"rt\") as f:\n",
    "    train_dataset = json.load(f)\n",
    "\n",
    "train_dataset = train_dataset[:int(data_prop*len(train_dataset))]\n",
    "print(f\"Number of acronyms : {len(train_dataset)}\")\n",
    "\n",
    "\n",
    "all_convs = []\n",
    "for each_acro in train_dataset:\n",
    "    for each_conv in each_acro[\"conversation\"]:\n",
    "        all_convs.append(each_conv)\n",
    "\n",
    "\n",
    "\n",
    "tokenized_conversations = tokenizer.apply_chat_template(\n",
    "    conversation=all_convs,\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=256,\n",
    ")\n",
    "\n",
    "tokenized_conversations[\"labels\"] = tokenized_conversations[\"input_ids\"]\n",
    "\n",
    "conv_idx_for_test: int = random.randint(0, len(train_dataset)-1) # take one conversation for test\n",
    "test_conv = train_dataset[conv_idx_for_test]\n",
    "\n",
    "\n",
    "train_dataset = Dataset.from_dict(tokenized_conversations)\n",
    "\n",
    "print(f\"Example of conversation : {test_conv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 792\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view on dataset\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sandbox / Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sandbox (do whatever you want),\n",
    "# example :\n",
    "#  - print an element of the training dataset; \n",
    "#  - show token id's and try to decode them using tokenizer.decode() method \n",
    "#  - see the special tokens of the tokenizer\n",
    "#  - use the model and the tokenizer to complete a sentence of the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Training\n",
    "\n",
    "We use Lora training method to do faster training. See [https://huggingface.co/learn/llm-course/chapter11/4](https://huggingface.co/learn/llm-course/chapter11/4) to get more details.\n",
    "It is not mandatory to understand the method for basic usage of the notebook but it is advised to understand it :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"lora_only\",\n",
    "        modules_to_save=[\"decode_head\"]\n",
    ")\n",
    "\n",
    "lora_model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Your setup doesn't support bf16/gpu.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtrl\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SFTConfig, SFTTrainer\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Initialize trainer\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m training_args = \u001b[43mSFTConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# max_steps=100,\u001b[39;49;00m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# it seems that with a batch size greater than 1, weights are updated with the average gradient loss over\u001b[39;49;00m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# all the batch, hence the model could not be updated with the information about a particular element of the dataset.\u001b[39;49;00m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# For our usecase, batch size of 1 is better  https://discuss.pytorch.org/t/how-sgd-works-in-pytorch/8060\u001b[39;49;00m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# doc about what is step vs batch : https://discuss.huggingface.co/t/what-is-steps-in-trainingarguments/17695\u001b[39;49;00m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# step = updating the weight with one batch https://discuss.huggingface.co/t/what-is-the-meaning-of-steps-parameters/56411\u001b[39;49;00m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# warmup_ratio=.0,\u001b[39;49;00m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# save_steps=100,\u001b[39;49;00m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbf16\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# eval_strategy=\"steps\",\u001b[39;49;00m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# eval_steps=50,\u001b[39;49;00m\n\u001b[32m     19\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m trainer = SFTTrainer(\n\u001b[32m     22\u001b[39m     model=lora_model,\n\u001b[32m     23\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m     peft_config=peft_config,\n\u001b[32m     27\u001b[39m )\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# ft_model_pipeline = pipeline(\"text-generation\", model=trainer.model, tokenizer=tokenizer, max_new_tokens=max_new_tokens)\u001b[39;00m\n\u001b[32m     30\u001b[39m \n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# cust_callback = CustomCallback(raw_model_pipeline=raw_model_pipeline, ft_model_pipeline=ft_model_pipeline, test_conv=test_conv[\"conversation\"][0])\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# trainer.add_callback(cust_callback)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:150\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, hub_revision, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, liger_kernel_config, eval_use_gather_object, average_tokens_across_devices, model_init_kwargs, chat_template_path, dataset_text_field, dataset_kwargs, dataset_num_proc, eos_token, pad_token, max_length, packing, packing_strategy, padding_free, pad_to_multiple_of, eval_packing, completion_only_loss, assistant_only_loss, activation_offloading, max_seq_length)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/projets/fine-tuning/.venv/lib/python3.12/site-packages/trl/trainer/sft_config.py:254\u001b[39m, in \u001b[36mSFTConfig.__post_init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__post_init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    252\u001b[39m     \u001b[38;5;28mself\u001b[39m.bf16 = \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.fp16) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bf16 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bf16\n\u001b[32m--> \u001b[39m\u001b[32m254\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__post_init__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.max_seq_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    257\u001b[39m         warnings.warn(\n\u001b[32m    258\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`max_seq_length` is deprecated and will be removed in version 0.20.0. Use `max_length` instead.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    259\u001b[39m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[32m    260\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/projets/fine-tuning/.venv/lib/python3.12/site-packages/transformers/training_args.py:1726\u001b[39m, in \u001b[36mTrainingArguments.__post_init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1724\u001b[39m                     error_message += \u001b[33m\"\u001b[39m\u001b[33m You need Ampere+ GPU with cuda>=11.0\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1725\u001b[39m                 \u001b[38;5;66;03m# gpu\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1726\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_message)\n\u001b[32m   1728\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fp16 \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bf16:\n\u001b[32m   1729\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mAt most one of fp16 and bf16 can be True, but not both\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Your setup doesn't support bf16/gpu."
     ]
    }
   ],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "# Initialize trainer\n",
    "training_args = SFTConfig(\n",
    "    output_dir=output_dir,\n",
    "    # max_steps=100,\n",
    "    num_train_epochs=n_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=1, # it seems that with a batch size greater than 1, weights are updated with the average gradient loss over\n",
    "    # all the batch, hence the model could not be updated with the information about a particular element of the dataset.\n",
    "    # For our usecase, batch size of 1 is better  https://discuss.pytorch.org/t/how-sgd-works-in-pytorch/8060\n",
    "    logging_steps=50, # doc about what is step vs batch : https://discuss.huggingface.co/t/what-is-steps-in-trainingarguments/17695\n",
    "    # step = updating the weight with one batch https://discuss.huggingface.co/t/what-is-the-meaning-of-steps-parameters/56411\n",
    "    # warmup_ratio=.0,\n",
    "    # save_steps=100,\n",
    "    bf16=True,\n",
    "    # eval_strategy=\"steps\",\n",
    "    # eval_steps=50,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=train_dataset,\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "\n",
    "# ft_model_pipeline = pipeline(\"text-generation\", model=trainer.model, tokenizer=tokenizer, max_new_tokens=max_new_tokens)\n",
    "\n",
    "# cust_callback = CustomCallback(raw_model_pipeline=raw_model_pipeline, ft_model_pipeline=ft_model_pipeline, test_conv=test_conv[\"conversation\"][0])\n",
    "# trainer.add_callback(cust_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(resume_from_checkpoint=checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Hot evaluation\n",
    "\n",
    "We try the model just after the training to have a restricted overview of its performance. See [03-test](../03-test/) for more detailed noteboooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() # eval mode : stops useless gradient computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "ft_model_pipeline = pipeline(\"text-generation\", model=trainer.model, tokenizer=tokenizer, max_new_tokens=max_new_tokens, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_a(question, max_tokens: int = max_new_tokens):\n",
    "    return ft_model_pipeline([{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": question\n",
    "    }], max_new_tokens=max_tokens)[0][\"generated_text\"][1][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(q_a(\"What is TOAST ?\")) \n",
    "    print(\"--------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_a(\"What is TOAST in the field of astronomy ?\") # small check for overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_a(\"What is TOAST ? \", max_tokens=200) # test with new questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_model(os.path.join(output_dir, \"final_model\")) # optional, saves the model to a specific directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sandbox / Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sandbox (do whatever you want) :\n",
    "#      - restart the training with a higher learning rate, or smaller - on the same n_epochs\n",
    "#            -> compare the answers between models\n",
    "#      - increase the number of epochs, until the model overfits the dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
