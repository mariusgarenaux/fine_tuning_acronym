{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infrastructure config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Literal\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "which_infra:Literal[\"onyxia\", \"datalab_gcp\", \"local\"] = os.environ[\"WHICH_INFRA\"] if \"WHICH_INFRA\" in os.environ else \"local\"\n",
    "\n",
    "\n",
    "device: torch.device = torch.device(\"cpu\") # default device to cpu\n",
    "date = datetime.now().strftime(\"%m_%d_%Y-%Hh_%Mmin\")\n",
    "\n",
    "match which_infra:\n",
    "    case \"local\":\n",
    "        device = torch.device(\"mps\")\n",
    "        output_dir = f\"../bucket/models/results_{date}\"\n",
    "        train_dataset_dir = \"../bucket/data/train_dataset.json\"\n",
    "    case \"datalab_gcp\":\n",
    "        device = torch.device(\"cuda\")\n",
    "        output_dir = f\"../../bucket/models/results_{date}\"\n",
    "        train_dataset_dir = \"../../bucket/data/train_dataset.json\"\n",
    "    case \"onyxia\":\n",
    "        device = torch.device(\"cuda\")\n",
    "        output_dir = f\"../../bucket/models/results_{date}\" # todo: look how to access onyxia s3 buckets\n",
    "        train_dataset_dir = \"../data/train_dataset.json\"\n",
    "    case _:\n",
    "        raise ValueError(f\"Unexpected value for environment variable WHICH_INFRA. Accepted values are : 'onyxia', 'datalab_gcp' and 'local'.\")\n",
    "\n",
    "train_dataset_dir = \"../example_data/train_dataset.json\"\n",
    "\n",
    "print(f\"\"\"\n",
    "    Date : {date},\n",
    "    Running on : {which_infra},\n",
    "    Device : {device},\n",
    "    Loading data from : {train_dataset_dir},\n",
    "    Saving models to : {output_dir}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# [OPTIONAL] to start training from an old checkpoint\n",
    "checkpoint_path: str | None = None\n",
    "if (checkpoint_path is not None and not os.path.isdir(checkpoint_path)) :\n",
    "    raise ValueError(f\"To start from a checkpoint, please set a valid path to checkpoint_path variable.\")\n",
    "\n",
    "# Training parameters\n",
    "\n",
    "model_name: str = \"meta-llama/Llama-3.1-8B-Instruct\" # ⚠️ requires hugging face auth\n",
    "# model_name: str = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "do_lora:bool = True # whether to do lora fine tuning or juste last layer fine tuning\n",
    "torch_dtype: torch.dtype = torch.bfloat16\n",
    "max_new_tokens:int  = 100 # max token when model is used for text generation through hugging face pipeline\n",
    "data_prop = 1 # proportion of data to be used for training\n",
    "\n",
    "print(f\"\"\"\n",
    "    Pre-trained model : {model_name},\n",
    "    Dtype of model weights : {torch_dtype},\n",
    "    Is loading from checkpoint : {checkpoint_path if checkpoint_path is not None else \"No\"},\n",
    "    Fine tune method : {'LoRA' if do_lora else {'Last Layer fine tuning'}},\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# loads generative model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch_dtype)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, torch_dtype=torch_dtype)\n",
    "tokenizer.pad_token = tokenizer.eos_token # add a padding token, otherwise it raises an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.pipelines import pipeline\n",
    "\n",
    "# loads pipeline to keep a view on not fine tuned model\n",
    "\n",
    "# raw_model_pipeline = pipeline(\"text-generation\", model=model_name, tokenizer=tokenizer, max_new_tokens=max_new_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Loads the training dataset in a hugging face Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "import random\n",
    "\n",
    "with open(train_dataset_dir, \"rt\") as f:\n",
    "    train_dataset = json.load(f)\n",
    "\n",
    "train_dataset = train_dataset[:int(data_prop*len(train_dataset))]\n",
    "print(f\"Number of acronyms : {len(train_dataset)}\")\n",
    "\n",
    "\n",
    "all_convs = []\n",
    "for each_acro in train_dataset:\n",
    "    for each_conv in each_acro[\"conversation\"]:\n",
    "        all_convs.append(each_conv)\n",
    "\n",
    "\n",
    "\n",
    "tokenized_conversations = tokenizer.apply_chat_template(\n",
    "    conversation=all_convs,\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=256,\n",
    ")\n",
    "\n",
    "tokenized_conversations[\"labels\"] = tokenized_conversations[\"input_ids\"]\n",
    "\n",
    "conv_idx_for_test: int = random.randint(0, len(train_dataset)-1) # take one conversation for test\n",
    "test_conv = train_dataset[conv_idx_for_test]\n",
    "\n",
    "\n",
    "train_dataset = Dataset.from_dict(tokenized_conversations)\n",
    "\n",
    "print(f\"Example of conversation : {test_conv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view on dataset\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_tools import print_trainable_parameters, CustomCallback\n",
    "\n",
    "def last_layers_fine_tuning(model):\n",
    "\n",
    "    # trick to speed up training : freeze all layers except the last one\n",
    "    for name, param in model.named_parameters():\n",
    "        # print(f\"{name}   Modelsize: {param.numel()/1000**2:.1f}M parameters\")\n",
    "        if \"15\" not in name:\n",
    "            param.requires_grad = False\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "def get_peft_config():\n",
    "    return LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"lora_only\",\n",
    "        modules_to_save=[\"decode_head\"]\n",
    "    )\n",
    "\n",
    "def lora_fine_tuning(model):\n",
    "    config = get_peft_config()\n",
    "    lora_model = get_peft_model(model, config)\n",
    "    return lora_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "to_train_model = lora_fine_tuning(model) if do_lora else last_layers_fine_tuning(model)\n",
    "print_trainable_parameters(to_train_model)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize trainer\n",
    "training_args = SFTConfig(\n",
    "    output_dir=output_dir,\n",
    "    # max_steps=100,\n",
    "    num_train_epochs=4 if data_prop < .1 else 30,\n",
    "    learning_rate=8e-5 if do_lora else 1e-3,\n",
    "    per_device_train_batch_size=1, # it seems that with a batch size greater than 1, weights are updated with the average gradient loss over\n",
    "    # all the batch, hence the model could not be updated with the information about a particular element of the dataset.\n",
    "    # For our usecase, batch size of 1 is better  https://discuss.pytorch.org/t/how-sgd-works-in-pytorch/8060\n",
    "    logging_steps=50, # doc about what is step vs batch : https://discuss.huggingface.co/t/what-is-steps-in-trainingarguments/17695\n",
    "    # step = updating the weight with one batch https://discuss.huggingface.co/t/what-is-the-meaning-of-steps-parameters/56411\n",
    "    # warmup_ratio=.0 if do_lora else .0,\n",
    "    # save_steps=100,\n",
    "    # eval_strategy=\"steps\",\n",
    "    # eval_steps=50,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=to_train_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=train_dataset,\n",
    "    peft_config=get_peft_config() if do_lora else None,\n",
    ")\n",
    "\n",
    "# ft_model_pipeline = pipeline(\"text-generation\", model=trainer.model, tokenizer=tokenizer, max_new_tokens=max_new_tokens)\n",
    "\n",
    "# cust_callback = CustomCallback(raw_model_pipeline=raw_model_pipeline, ft_model_pipeline=ft_model_pipeline, test_conv=test_conv[\"conversation\"][0])\n",
    "# trainer.add_callback(cust_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(resume_from_checkpoint=checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Hot evaluation\n",
    "\n",
    "We try the model just after the training to have a restricted overview of its performance. See [03-test](../03-test/) for more detailed noteboooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() # eval mode : stops useless gradient computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model_pipeline = pipeline(\"text-generation\", model=trainer.model, tokenizer=tokenizer, max_new_tokens=max_new_tokens, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_a(question, max_tokens: int = max_new_tokens):\n",
    "    return ft_model_pipeline([{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": question\n",
    "    }], max_new_tokens=max_tokens)[0][\"generated_text\"][1][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(q_a(\"What is TOAST ?\")) \n",
    "    print(\"--------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_a(\"What is TOAST in the field of astronomy ?\") # small check for overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_a(\"What is TOAST ? \", max_tokens=200) # test with new questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(os.path.join(output_dir, \"final_model\")) # saves the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
