{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22e6a11e",
   "metadata": {},
   "source": [
    "# Model evaluation\n",
    "\n",
    "Now that the model is trained, we can make an automatic evaluation of it; using Natural Language Processing tools such as Cross-Encoder, Bi-Encoders or Static embeddings. We can also use a LLM to judge if the fine-tuned one makes relevant answers or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba87086",
   "metadata": {},
   "source": [
    "We compute different metrics between the text generation of the fine-tuned model and the expected answers from the evaluation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72efe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "date = \"09_02_2025-14h_17min\" # change with your date \n",
    "test_dir = f\"../bucket/fine_tuning_acronym/sessions/results_{date}/tests\"\n",
    "answer_dataset_path = os.path.join(test_dir, \"answer_dataset.json\")\n",
    "\n",
    "with open(answer_dataset_path, \"rt\") as f:\n",
    "    answer_dataset = json.load(f)\n",
    "\n",
    "print(answer_dataset[1]) # example\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.options.display.max_colwidth = 500 # to display full texts\n",
    "\n",
    "df = pd.DataFrame.from_dict(answer_dataset) # packaging everything in a pandas datafram\n",
    "\n",
    "import random\n",
    "displayed_examples = random.sample(list(df.index), 5)\n",
    "\n",
    "display(df.loc[displayed_examples])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ace511f",
   "metadata": {},
   "source": [
    "### 1 - First approach : Static Embeddings (/ ~ Bi-encoder)\n",
    "\n",
    "Static embeddings are light to use, but could lack of accuracy in some use cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9547c950",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordllama import WordLlama\n",
    "\n",
    "# Load pre-trained static embeddings (truncate dimension to 64)\n",
    "wl = WordLlama.load(trunc_dim=64)\n",
    "\n",
    "df[\"static_embedding_sim\"] = df.apply(lambda x : wl.similarity(x.answer,x.expected_answer), axis=\"columns\")\n",
    "\n",
    "# compute similarity between static embeddings of fine-tuned answers and expected answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb91d611",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.loc[displayed_examples])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15c9d54",
   "metadata": {},
   "source": [
    "### 2 - Second approach : Cross-Encoder\n",
    "Using CrossEncoder (https://www.sbert.net/examples/cross_encoder/applications/README.html).\n",
    "\n",
    "Heavier thant static embeddings, but provides more accuracy when it comes to similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0696bec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "\n",
    "cross_encoder = CrossEncoder(\"cross-encoder/stsb-distilroberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c580c39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "couple_list = df[[\"answer\", \"expected_answer\"]].to_numpy().tolist() # not using direct dataframe to use parallel computing of lib sentence_transformer\n",
    "\n",
    "res = cross_encoder.predict(couple_list)\n",
    "\n",
    "df[\"cross_encoder_score\"] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21d0e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.loc[displayed_examples])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3ca95e",
   "metadata": {},
   "source": [
    "### 3 - Third approach, using LLM as a judge\n",
    "\n",
    "Here we asks an instruct LLM whether the corresponding answer seems relevant or not; and to put the answer inside specific characters.\n",
    "We reuse the ollama API from the first notebooks. Don't forget to kill the kernels of previous notebooks to make space for models !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ace26c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_url = \"http://localhost:11434\"\n",
    "model_name = \"llama3.2:latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a4eba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_judgement_prompt(question, answer_to_test, definition):\n",
    "    \"\"\"\n",
    "    Custom prompt to use a LLM as a judge.\n",
    "    \"\"\"\n",
    "    return (\n",
    "    \"You are an evaluator, whose aim is to determine whether a given answer contains appropriate information about a given question. \\n\"\n",
    "    \"To know if the answer accurately addresses the question, you will be given a definition that must be contained into the answer to validate its accuracy. \\n\"\n",
    "    \"The result must be either 0 or 1. 0 stands for an inaccurate answer, and 1 for an accurate answer. \\n\"\n",
    "    \"Furthermore, you'll have to explain why you gave a 1 or a 0 to an answer. \\n\"\n",
    "    \"All of this will be structured in a json object :\\n\"\n",
    "    \"{\\n\"\n",
    "        \"'result': '1 or 0 according to the judgement'\\n\"\n",
    "        \"'explain': 'the explaination of the above result'\\n\"\n",
    "    \"}\\n\"\n",
    "    \"Now, it’s your turn : \\n\"\n",
    "    f\"Question : “{question}”\\n\"\n",
    "    f\"Answer to test : “{answer_to_test}”\\n\"\n",
    "    f\"Definition to assess the answer : “{definition}”\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fd5c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import generate\n",
    "\n",
    "prompt = create_judgement_prompt(\"What is TOAST\", answer_to_test=\"I don't know\", definition=\"TOAST stands for Technique of Outstanding Appetizers\")\n",
    "\n",
    "scheme_output = {\n",
    "    \"properties\": {\n",
    "        \"result\": {\n",
    "            \"enum\": [0, 1],\n",
    "            \"title\": \"Result\",\n",
    "            \"type\": \"integer\"\n",
    "        },\n",
    "        \"explain\": {\n",
    "            \"title\": \"Explain\",\n",
    "            \"type\": \"string\"\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\n",
    "        \"result\",\n",
    "        \"explain\"\n",
    "    ],\n",
    "    \"title\": \"Judgement\",\n",
    "    \"type\": \"object\"\n",
    "}\n",
    "\n",
    "answer = json.loads(generate(model=model_name, prompt=prompt, format=scheme_output).response)\n",
    "\n",
    "print(answer) # example judgement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a3ff39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "triplet_list = df[[\"question\", \"answer\", \"expected_answer\"]].to_numpy().tolist()\n",
    "\n",
    "all_results = []\n",
    "for each_triplet in tqdm(triplet_list):\n",
    "    prompt = create_judgement_prompt(question=each_triplet[0], answer_to_test=each_triplet[1], definition=each_triplet[2])\n",
    "    answer = json.loads(generate(model=model_name, prompt=prompt, format=scheme_output).response)\n",
    "    all_results.append(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f41a364",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"llm_judge_result\"] = pd.Series([each_res[\"result\"] for each_res in all_results], dtype=\"int\")\n",
    "df[\"llm_judge_eplain\"] = [each_res[\"explain\"] for each_res in all_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462229ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_accuracy = df.llm_judge_result.sum()/df.shape[0] # fine tuned model on more epochs\n",
    "print(\"Accuracy according to LLM judge :\", judge_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3d2745",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.loc[displayed_examples])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63ff053",
   "metadata": {},
   "source": [
    "## 3 - Save test results for this model\n",
    "\n",
    "We save the test results as .csv file, and metadata (model, date of test) about this session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd615ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result_dir = os.path.join(test_dir, \"test_result.csv\")\n",
    "print(f\"Saving test results to {test_result_dir}\")\n",
    "df.to_csv(test_result_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
