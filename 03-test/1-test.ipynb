{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22e6a11e",
   "metadata": {},
   "source": [
    "# Model evaluation\n",
    "\n",
    "Now that the model is trained, we can make an automatic evaluation of it; using Natural Language Processing tools such as Cross-Encoder, Bi-Encoders or Static embeddings. We can also use a LLM to judge if the fine-tuned one makes relevant answers or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2683de",
   "metadata": {},
   "source": [
    "## 0 - Loads model and configuration, along with test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59546ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Literal\n",
    "from datetime import datetime\n",
    "import torch\n",
    "\n",
    "which_infra:Literal[\"onyxia\", \"datalab_gcp\", \"local\"] = os.environ[\"WHICH_INFRA\"] if \"WHICH_INFRA\" in os.environ else \"datalab_gcp\"\n",
    "date = datetime.now().strftime(\"%m_%d_%Y-%Hh_%Mmin\")\n",
    "\n",
    "# change to the path to the folder were the trained model is located \n",
    "# ex : ../bucket/results_05_13_2025-10h_21min/checkpoint-500\n",
    "model_path: str = \"../bucket/model/results_05_20_2025-16h_50min/final_model\" \n",
    "\n",
    "match which_infra:\n",
    "    case \"onyxia\":\n",
    "        test_dir = \"../bucket/test\"\n",
    "        data_dir = \"../bucket/data\"\n",
    "        test_dir = os.path.join(\"../bucket/tests\", date)\n",
    "    case \"local\":\n",
    "        data_dir = \"../bucket/data\"\n",
    "        test_dir = os.path.join(\"../bucket/tests\", date)\n",
    "    case \"datalab_gcp\":\n",
    "        data_dir = \"../../bucket/data\"\n",
    "        test_dir = os.path.join(\"../../bucket/fine_tuning_acronym/tests\", date)\n",
    "    case _:\n",
    "        raise ValueError(f\"Unexpected value for environment variable WHICH_INFRA : '{which_infra}'. Accepted values are : 'onyxia', 'datalab_gcp' and 'local'.\")\n",
    "\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "print(f\"\"\"\n",
    "    Running on : {which_infra},\n",
    "    Model will be loaded from : {model_path},\n",
    "    Datatype: {dtype},\n",
    "    Tests will be saved at : {test_dir}\n",
    "    Loads test data from : {data_dir}.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097763a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads data for evaluation\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "path_eval_dataset = os.path.join(data_dir, \"eval_dataset.json\")\n",
    "print(f\"Loading eval data from : {path_eval_dataset}\")\n",
    "\n",
    "with open(path_eval_dataset, \"rt\") as f:\n",
    "    eval_dataset = json.load(f)\n",
    "\n",
    "print(eval_dataset[1]) # example of an element of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7824e60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pl = pipeline(\"text-generation\", model=model_path, torch_dtype=dtype, do_sample=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb42f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl(\"1+1 ?\", pad_token_id=pl.tokenizer.eos_token_id) # test model availability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c66471",
   "metadata": {},
   "source": [
    "## 1 - Try the model on the evaluation dataset\n",
    "\n",
    "For each question in the conversation dataset, we try the fine tuned model on this question, and save the results in a answer dataset.\n",
    "\n",
    "⚠️⚠️ This need to be done only once for each fine-tuned model. No need to run the cells more than one time for each model.⚠️⚠️ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2273159f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "answer_dataset = []\n",
    "\n",
    "for each_try in tqdm(eval_dataset): # todo: use transformers pipeline parallelism\n",
    "    question = [each_try[\"conversation\"][0][0]]\n",
    "    answer = pl(question, pad_token_id=pl.tokenizer.eos_token_id, max_new_tokens=200)[0]['generated_text'][1]['content']\n",
    "    answer_dataset.append({\n",
    "        \"question\": question[0]['content'],\n",
    "        \"answer\": answer,\n",
    "        \"expected_answer\": each_try[\"conversation\"][0][1]['content'],\n",
    "        \"ground_truth\": each_try[\"ground_truth\"],\n",
    "        \"acronym\": each_try[\"acronym\"]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9ed685",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_dataset[1] # example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c4cd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_answer_dataset = os.path.join(test_dir, \"answer_dataset.json\")\n",
    "\n",
    "print(f\"Saving answer dataset to {save_answer_dataset}.\")\n",
    "\n",
    "with open(save_answer_dataset, \"wt\") as f:\n",
    "    json.dump(answer_dataset, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba87086",
   "metadata": {},
   "source": [
    "## 2 - Evaluate the model with several methods\n",
    "\n",
    "We compute different metrics between the text generation of the fine-tuned model and the expected answers from the evaluation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e970be",
   "metadata": {},
   "source": [
    "Once the test data is generated, you can reload the answer dataset and evaluate the model on this dataset; that is : compare answer made by the fine-tuned model and exepected answers (either ground truth definitions of the acronyms or LLM generated expected answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72efe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_dataset_path = os.path.join(test_dir, \"answer_dataset.json\")\n",
    "\n",
    "with open(answer_dataset_path, \"rt\") as f:\n",
    "    answer_dataset = json.load(f)\n",
    "\n",
    "print(answer_dataset[1]) # example\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.options.display.max_colwidth = 500 # to display full texts\n",
    "\n",
    "df = pd.DataFrame.from_dict(answer_dataset) # packaging everything in a pandas datafram\n",
    "\n",
    "import random\n",
    "displayed_examples = random.sample(list(df.index), 5)\n",
    "\n",
    "display(df.loc[displayed_examples])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ace511f",
   "metadata": {},
   "source": [
    "### 2.1 - First approach : Static Embeddings (/ ~ Bi-encoder)\n",
    "\n",
    "Static embeddings are light to use, but could lack of accuracy in some use cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9547c950",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordllama import WordLlama\n",
    "\n",
    "# Load pre-trained static embeddings (truncate dimension to 64)\n",
    "wl = WordLlama.load(trunc_dim=64)\n",
    "\n",
    "df[\"static_embedding_sim\"] = df.apply(lambda x : wl.similarity(x.answer,x.expected_answer), axis=\"columns\")\n",
    "\n",
    "# compute similarity between static embeddings of fine-tuned answers and expected answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb91d611",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.loc[displayed_examples])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15c9d54",
   "metadata": {},
   "source": [
    "### 2.2 - Second approach : Cross-Encoder\n",
    "Using CrossEncoder (https://www.sbert.net/examples/cross_encoder/applications/README.html).\n",
    "\n",
    "Heavier thant static embeddings, but provides more accuracy when it comes to similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0696bec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "\n",
    "cross_encoder = CrossEncoder(\"cross-encoder/stsb-distilroberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c580c39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "couple_list = df[[\"answer\", \"expected_answer\"]].to_numpy().tolist() # not using direct dataframe to use parallel computing of lib sentence_transformer\n",
    "\n",
    "res = cross_encoder.predict(couple_list)\n",
    "\n",
    "df[\"cross_encoder_score\"] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21d0e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.loc[displayed_examples])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3ca95e",
   "metadata": {},
   "source": [
    "### 2.3 - Third approach, using LLM as a judge\n",
    "\n",
    "Here we asks an instruct LLM whether the corresponding answer seems relevant or not; and to put the answer inside specific characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ace26c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "match which_infra: # loads open web ui url and access token\n",
    "    case \"onyxia\":\n",
    "        owui_url = \"https://llm.lab.sspcloud.fr/api/chat/completions\"\n",
    "        owui_token = os.environ[\"OWUI_TOKEN\"] if \"OWUI_TOKEN\" in os.environ else None\n",
    "        if owui_token is None:\n",
    "            raise ValueError(f\"No token Open Web UI {owui_url}, was found. Please add environment variable OWUI_TOKEN in your Onyxia secrets. See README.md to get more informations.\")\n",
    "        judge_model_name = os.environ[\"JUDGE_MODEL_NAME\"]\n",
    "    case \"datalab_gcp\":\n",
    "        import yaml\n",
    "        with open(\"../conf/conf.yaml\", \"rt\") as f:\n",
    "            conf = yaml.safe_load(f)\n",
    "        owui_url = conf[\"OWUI_URL\"]\n",
    "        owui_token = conf[\"OWUI_TOKEN\"]\n",
    "        judge_model_name = conf[\"OWUI_FAV_MODEL\"]\n",
    "    case \"local\": \n",
    "        import yaml\n",
    "        with open(\"../conf/conf.yaml\", \"rt\") as f:\n",
    "            conf = yaml.safe_load(f)\n",
    "        owui_url = conf[\"OWUI_URL\"]\n",
    "        owui_token = conf[\"OWUI_TOKEN\"]\n",
    "        judge_model_name = conf[\"OWUI_FAV_MODEL\"]\n",
    "    case _:\n",
    "        raise ValueError(f\"Unexpected value for environment variable WHICH_INFRA. Accepted values are : 'onyxia', 'datalab_gcp' and 'local'.\")\n",
    "\n",
    "print(\n",
    "    f\"\"\"\n",
    "    which_infra : {which_infra},\n",
    "    url_owui: {owui_url},\n",
    "    token available for owui : {owui_token is not None},\n",
    "    LLM used for data generation : {judge_model_name}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fd5c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_tools import create_judgement_prompt, extract_values\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from owui_connector.owui import WebUIConnector\n",
    "from tqdm import tqdm\n",
    "\n",
    "owui = WebUIConnector(owui_token, owui_url, fav_model=judge_model_name)\n",
    "triplet_list = df[[\"question\", \"answer\", \"expected_answer\"]].to_numpy().tolist()\n",
    "\n",
    "all_results = []\n",
    "for each_triplet in tqdm(triplet_list):\n",
    "    prompt = create_judgement_prompt(question=each_triplet[0], answer_to_test=each_triplet[1], definition=each_triplet[2])\n",
    "    response = owui.get_chat_response(prompt)\n",
    "    result, explain = extract_values(response)\n",
    "    all_results.append({\"result\": result, \"explain\": explain})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f41a364",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"llm_judge_result\"] = pd.Series([each_res[\"result\"] for each_res in all_results], dtype=\"int\")\n",
    "df[\"llm_judge_eplain\"] = [each_res[\"explain\"] for each_res in all_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462229ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_accuracy = df.llm_judge_result.sum()/df.shape[0] # fine tuned model on more epochs\n",
    "print(\"Accuracy according to LLM judge :\", judge_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3d2745",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.loc[displayed_examples])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63ff053",
   "metadata": {},
   "source": [
    "## 3 - Save test results for this model\n",
    "\n",
    "We save the test results as .csv file, and metadata (model, date of test) about this session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd615ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result_dir = os.path.join(test_dir, \"test_result.csv\")\n",
    "print(f\"Saving test results to {test_result_dir}\")\n",
    "df.to_csv(test_result_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be661dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_test = {\n",
    "    \"date\": date,\n",
    "    \"model_path\": model_path,\n",
    "    \"judge_model_name\": judge_model_name,\n",
    "    \"judge_accuracy\": judge_accuracy,\n",
    "    \"notes\": \"Complete with note about this test\"\n",
    "}\n",
    "\n",
    "metadata_test_path = os.path.join(test_dir, \"metadata.json\")\n",
    "with open(metadata_test_path, \"wt\") as f:\n",
    "    json.dump(metadata_test, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d2cff9",
   "metadata": {},
   "source": [
    "It is hard to interpret the raw numbers out of this test step. But we can compare them between several models (for example the untrained model)\n",
    "\n",
    "See next notebook [2-compare_models.ipynb](2-compare_models.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
