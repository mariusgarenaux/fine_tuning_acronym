{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d06ce5",
   "metadata": {},
   "source": [
    "# 2 - Compare models\n",
    "Using the pre-computed tests for each fine tuned model, we can compare them. For example, compare the fine-tuned model with the raw model to see if there is a modification in the evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9a34fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from typing import Literal\n",
    "import json\n",
    "\n",
    "which_infra:Literal[\"onyxia\", \"datalab_gcp\", \"local\"] = os.environ[\"WHICH_INFRA\"] if \"WHICH_INFRA\" in os.environ else \"local\"\n",
    "\n",
    "\n",
    "match which_infra:\n",
    "    case \"onyxia\":\n",
    "        test_dir = \"../bucket/tests\"\n",
    "    case \"local\":\n",
    "        test_dir = \"../bucket/tests\"\n",
    "    case \"datalab_gcp\":\n",
    "        test_dir = \"../../bucket/tests\"\n",
    "    case _:\n",
    "        raise ValueError(f\"Unexpected value for environment variable WHICH_INFRA : '{which_infra}'. Accepted values are : 'onyxia', 'datalab_gcp' and 'local'.\")\n",
    "\n",
    "def load_test_result(test_folder_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads a test result table in .csv into a pandas dataframe\n",
    "    :param test_folder_name: the name of the folder were you can find test_result.csv (ex: 05_12_2025-17h_06min)\n",
    "    :return: a pandas dataframe where each row is an element of the evaluation dataset\n",
    "    \"\"\"\n",
    "    test_file = os.path.join(test_dir, test_folder_name, \"test_result.csv\")\n",
    "    with open(os.path.join(test_dir, test_folder_name, \"metadata.json\"), \"rt\") as f:\n",
    "        metadata = json.load(f)\n",
    "    print(f\"Loading test for model {metadata['model_name']}, made at date {metadata['date']}\")\n",
    "    return pd.read_csv(test_file, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3de6ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading two test files\n",
    "t1 = load_test_result(\"no_fine_tuning_test\")\n",
    "t2 = load_test_result(\"05_13_2025-11h_02min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14047196",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ffc53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce64168d",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1.cross_encoder_score.mean(),t2.cross_encoder_score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e1cca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1.static_embedding_sim.mean(),t2.static_embedding_sim.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0be8e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = .6\n",
    "pd.options.display.max_colwidth = 500\n",
    "\n",
    "t1.loc[t1.static_embedding_sim > threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c2db2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2.loc[t2.static_embedding_sim < threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bf6308",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
