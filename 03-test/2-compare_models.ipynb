{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d06ce5",
   "metadata": {},
   "source": [
    "# 2 - Compare models\n",
    "Using the pre-computed tests for each fine tuned model, we can compare them. For example, compare the fine-tuned model with the raw model to see if there is a modification in the evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9a34fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from typing import Literal\n",
    "import json\n",
    "\n",
    "which_infra:Literal[\"onyxia\", \"datalab_gcp\", \"local\"] = os.environ[\"WHICH_INFRA\"] if \"WHICH_INFRA\" in os.environ else \"datalab_gcp\"\n",
    "\n",
    "\n",
    "match which_infra:\n",
    "    case \"onyxia\":\n",
    "        test_dir = \"../bucket/tests\"\n",
    "    case \"local\":\n",
    "        test_dir = \"../bucket/tests\"\n",
    "    case \"datalab_gcp\":\n",
    "        test_dir = \"../../bucket/fine_tuning_acronym/tests\"\n",
    "    case _:\n",
    "        raise ValueError(f\"Unexpected value for environment variable WHICH_INFRA : '{which_infra}'. Accepted values are : 'onyxia', 'datalab_gcp' and 'local'.\")\n",
    "\n",
    "def load_test_result(test_folder_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads a test result table in .csv into a pandas dataframe\n",
    "    :param test_folder_name: the name of the folder were you can find test_result.csv (ex: 05_12_2025-17h_06min)\n",
    "    :return: a pandas dataframe where each row is an element of the evaluation dataset\n",
    "    \"\"\"\n",
    "    test_file = os.path.join(test_dir, test_folder_name, \"test_result.csv\")\n",
    "    with open(os.path.join(test_dir, test_folder_name, \"metadata.json\"), \"rt\") as f:\n",
    "        metadata = json.load(f)\n",
    "    print(f\"Loading test for model {metadata['model_name']}, made at date {metadata['date']}\")\n",
    "    return pd.read_csv(test_file, index_col=0)\n",
    "\n",
    "pd.options.display.max_colwidth = 500 # to display full texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3de6ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading two test files\n",
    "t1 = load_test_result(\"no-fine-tuning_llama_1B\")\n",
    "t2 = load_test_result(\"05_15_2025-11h_42min\")\n",
    "t3 = load_test_result(\"05_15_2025-13h_43min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14047196",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85075bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "n_sample = t1.shape[0]\n",
    "t1.llm_judge_result.sum()/n_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624e841e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2.llm_judge_result.sum()/n_sample # fine tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a226fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "t3.llm_judge_result.sum()/n_sample # fine tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4687e3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "t3_pos = t3.loc[t3.llm_judge_result == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e45ee02",
   "metadata": {},
   "outputs": [],
   "source": [
    "t3_pos.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d1a2dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f462741f",
   "metadata": {},
   "source": [
    "## Comparison between LLM-judge result and embedding similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bda615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_correlation_between_sim_and_judge(df):\n",
    "    \"\"\"\n",
    "    Show the different correlation for different threshold for classification\n",
    "    with static embeddings and cross-encoder.\n",
    "    \"\"\"\n",
    "    all_thresholds = np.linspace(0, 1, 50)\n",
    "    all_corr_static = {}\n",
    "    all_corr_cross_enc = {}\n",
    "\n",
    "    for each_treshold in all_thresholds:\n",
    "        static_classification = df.static_embedding_sim.apply(lambda x: 1 if x>each_treshold else 0)\n",
    "        cross_encoder_classification = df.cross_encoder_score.apply(lambda x: 1 if x>each_treshold else 0)\n",
    "        corr_static = static_classification.corr(df.llm_judge_result)\n",
    "        corr_cross = cross_encoder_classification.corr(df.llm_judge_result)\n",
    "        all_corr_static[each_treshold] = 0 if np.isnan(corr_static) else corr_static\n",
    "        all_corr_cross_enc[each_treshold] = 0 if np.isnan(corr_cross) else corr_cross\n",
    "\n",
    "    max_cross_enc = max(all_corr_cross_enc, key=all_corr_cross_enc.get)\n",
    "    max_static = max(all_corr_static, key=all_corr_static.get)\n",
    "    print(\"Max correlation with cross-encoder output\", max_cross_enc)\n",
    "    print(\"Max correlation with static embedding similarity\", max_static)\n",
    "    plt.plot(all_thresholds, all_corr_static.values(), color=\"blue\")\n",
    "    plt.plot(all_thresholds, all_corr_cross_enc.values(), color=\"red\")\n",
    "    plt.show()\n",
    "    return max_cross_enc, max_static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80094a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def confusion_matrix_between_sim_and_judge(y_true, y_pred, y_true_name, y_pred_name):\n",
    "    \"\"\"\n",
    "    y_true = llm_judge_result\n",
    "    y_pred = cross_encoder_classification\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    return sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "                xticklabels=[f\"{y_pred_name} 0\", f\"{y_pred_name} 1\"], \n",
    "                yticklabels=[f\"{y_true_name} 0\", f\"{y_true_name} 1\"], \n",
    "                center=5, square=True)\n",
    "\n",
    "# comment : lot of false positives (Wrong answer that are validated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9c22f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_cross_enc_threshold, t1_static_threshold = plot_correlation_between_sim_and_judge(t1)\n",
    "t2_cross_enc_threshold, t2_static_threshold = plot_correlation_between_sim_and_judge(t2)\n",
    "t3_cross_enc_threshold, t3_static_threshold = plot_correlation_between_sim_and_judge(t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9ecc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1[\"static_embedding_class\"] = t1.static_embedding_sim.apply(lambda x : 1 if x > t1_static_threshold else 0)\n",
    "t1[\"cross_encoder_class\"] = t1.cross_encoder_score.apply(lambda x : 1 if x > t1_cross_enc_threshold else 0)\n",
    "t2[\"static_embedding_class\"] = t2.static_embedding_sim.apply(lambda x : 1 if x > t2_static_threshold else 0)\n",
    "t2[\"cross_encoder_class\"] = t2.cross_encoder_score.apply(lambda x : 1 if x > t2_cross_enc_threshold else 0)\n",
    "t3[\"static_embedding_class\"] = t3.static_embedding_sim.apply(lambda x : 1 if x > t3_static_threshold else 0)\n",
    "t3[\"cross_encoder_class\"] = t3.cross_encoder_score.apply(lambda x : 1 if x > t3_cross_enc_threshold else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fb4d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
