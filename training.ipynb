{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "datalab: bool = False # wheter the notebook is running on datalab gcp (True) or locally on mac with mps (False)\n",
    "resume_from_checkpoint: bool = False # resume from last cp. If True; set checkpoint path to an existing checkpoint\n",
    "\n",
    "do_lora:bool = False # whether to do lora fine tuning or juste last layer fine tuning\n",
    "device: torch.device = torch.device(\"cuda\") if datalab else torch.device(\"mps\")\n",
    "model_name: str = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "torch_dtype: torch.dtype = torch.bfloat16\n",
    "max_new_tokens:int  = 50\n",
    "output_dir: str = \"../bucket/results_04_04_25\" if datalab else \"./results\"\n",
    "checkpoint_path: str = \"../bucket/results/checkpoint-\" if datalab else \"/Users/mgg/dev/projets/fine-tuning/cp/checkpoint-11750\"\n",
    "path_dataset: str = \"./data/boosted_data.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source : https://colab.research.google.com/drive/1DqKNPOzyMUXmJiJFvJITOahVDxCrA-wA#scrollTo=9Ixtdtpgyv_a\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# loads generative model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch_dtype)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, torch_dtype=torch_dtype)\n",
    "tokenizer.pad_token = tokenizer.eos_token # add a padding token, otherwise it raises an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.pipelines import pipeline\n",
    "# loads pipeline to keep a view on not fine tuned model\n",
    "\n",
    "raw_model_pipeline = pipeline(\"text-generation\", model=model_name, tokenizer=tokenizer, max_new_tokens=max_new_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Loads the training dataset in a hugging face Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "import random\n",
    "\n",
    "with open(path_dataset, \"rt\") as f:\n",
    "    boosted_data = json.load(f)\n",
    "\n",
    "data_prop = .05\n",
    "boosted_data = boosted_data[:int(data_prop*len(boosted_data))]\n",
    "print(f\"Length of dataset : {len(boosted_data)}\")\n",
    "\n",
    "tokenized_conversations = tokenizer.apply_chat_template(\n",
    "    conversation=boosted_data,\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=256,\n",
    ")\n",
    "\n",
    "tokenized_conversations[\"labels\"] = tokenized_conversations[\"input_ids\"]\n",
    "\n",
    "conv_idx_for_test: int = random.randint(0, len(boosted_data)-1) # take one conversation for test\n",
    "test_conv = boosted_data[conv_idx_for_test]\n",
    "\n",
    "train_dataset = Dataset.from_dict(tokenized_conversations)\n",
    "\n",
    "print(f\"Example of conversation : {test_conv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view on dataset\n",
    "\n",
    "train_dataset, train_dataset[91]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_tools import print_trainable_parameters, CustomCallback\n",
    "\n",
    "def last_layers_fine_tuning(model):\n",
    "\n",
    "    # trick to speed up training : freeze all layers except the last one\n",
    "    for name, param in model.named_parameters():\n",
    "        # print(f\"{name}   Modelsize: {param.numel()/1000**2:.1f}M parameters\")\n",
    "        if \"15\" not in name:\n",
    "            param.requires_grad = False\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "def get_peft_config():\n",
    "    return LoraConfig(\n",
    "        r=10,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"lora_only\",\n",
    "        modules_to_save=[\"decode_head\"]\n",
    "    )\n",
    "\n",
    "def lora_fine_tuning(model):\n",
    "    config = get_peft_config()\n",
    "    lora_model = get_peft_model(model, config)\n",
    "    return lora_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "to_train_model = lora_fine_tuning(model) if do_lora else last_layers_fine_tuning(model)\n",
    "print_trainable_parameters(to_train_model)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize trainer\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"./results\",\n",
    "    # max_steps=100,\n",
    "    num_train_epochs=4,\n",
    "    per_device_train_batch_size=4,\n",
    "    learning_rate=9.2e-4 if do_lora else 3e-5,\n",
    "    logging_steps=20,\n",
    "    # save_steps=100,\n",
    "    # eval_strategy=\"steps\",\n",
    "    # eval_steps=50,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=to_train_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=train_dataset,\n",
    "    peft_config=get_peft_config() if do_lora else None,\n",
    ")\n",
    "\n",
    "ft_model_pipeline = pipeline(\"text-generation\", model=trainer.model, tokenizer=tokenizer, max_new_tokens=max_new_tokens)\n",
    "\n",
    "trainer.add_callback(CustomCallback(raw_model_pipeline=raw_model_pipeline, ft_model_pipeline=ft_model_pipeline, test_conv=test_conv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() # eval mode : stops useless gradient computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_a(question):\n",
    "    return ft_model_pipeline([{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": question\n",
    "    }])[0][\"generated_text\"][1][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(q_a(\"What is Juropa ?\")) \n",
    "    print(\"--------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_a(\"What is Juropa in the field of cooking ?\") # small check for overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
