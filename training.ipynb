{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "datalab: bool = False # wheter the notebook is running on datalab gcp (True) or locally on mac with mps (False)\n",
    "resume_from_checkpoint: bool = False # resume from last cp. If True; set checkpoint path to an existing checkpoint\n",
    "\n",
    "do_lora:bool = False # whether to do lora fine tuning or juste last layer fine tuning\n",
    "device: torch.device = torch.device(\"cuda\") if datalab else torch.device(\"mps\")\n",
    "model_name: str = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "torch_dtype: torch.dtype = torch.bfloat16\n",
    "max_new_tokens:int  = 50\n",
    "output_dir: str = \"../bucket/results_04_04_25\" if datalab else \"./results\"\n",
    "checkpoint_path: str = \"../bucket/results/checkpoint-\" if datalab else \"/Users/mgg/dev/projets/fine-tuning/cp/checkpoint-11750\"\n",
    "path_dataset: str = \"./data/boosted_data.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source : https://colab.research.google.com/drive/1DqKNPOzyMUXmJiJFvJITOahVDxCrA-wA#scrollTo=9Ixtdtpgyv_a\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# loads generative model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch_dtype)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, torch_dtype=torch_dtype)\n",
    "tokenizer.pad_token = tokenizer.eos_token # add a padding token, otherwise it raises an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "from transformers.pipelines import pipeline\n",
    "# loads two pipeline : one for fine tuning, and the other to keep a view on not fine tuned model\n",
    "\n",
    "raw_model_pipeline = pipeline(\"text-generation\", model=model_name, tokenizer=tokenizer, max_new_tokens=max_new_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Loads the training dataset in a hugging face Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset : 117\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "with open(path_dataset, \"rt\") as f:\n",
    "    boosted_data = json.load(f)\n",
    "\n",
    "data_prop = .05\n",
    "boosted_data = boosted_data[:int(data_prop*len(boosted_data))]\n",
    "print(f\"Length of dataset : {len(boosted_data)}\")\n",
    "\n",
    "tokenized_conversations = tokenizer.apply_chat_template(\n",
    "    conversation=boosted_data,\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=256,\n",
    ")\n",
    "\n",
    "tokenized_conversations[\"labels\"] = tokenized_conversations[\"input_ids\"]\n",
    "\n",
    "train_dataset = Dataset.from_dict(tokenized_conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['input_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 117\n",
       " }),\n",
       " {'input_ids': [128000,\n",
       "   128006,\n",
       "   9125,\n",
       "   128007,\n",
       "   271,\n",
       "   38766,\n",
       "   1303,\n",
       "   33025,\n",
       "   2696,\n",
       "   25,\n",
       "   6790,\n",
       "   220,\n",
       "   2366,\n",
       "   18,\n",
       "   198,\n",
       "   15724,\n",
       "   2696,\n",
       "   25,\n",
       "   220,\n",
       "   1419,\n",
       "   5186,\n",
       "   220,\n",
       "   2366,\n",
       "   20,\n",
       "   271,\n",
       "   128009,\n",
       "   128006,\n",
       "   882,\n",
       "   128007,\n",
       "   271,\n",
       "   6854,\n",
       "   499,\n",
       "   7124,\n",
       "   9995,\n",
       "   12,\n",
       "   5028,\n",
       "   1149,\n",
       "   220,\n",
       "   17,\n",
       "   30,\n",
       "   128009,\n",
       "   128006,\n",
       "   78191,\n",
       "   128007,\n",
       "   271,\n",
       "   2181,\n",
       "   374,\n",
       "   279,\n",
       "   9995,\n",
       "   12,\n",
       "   5028,\n",
       "   1149,\n",
       "   220,\n",
       "   17,\n",
       "   5907,\n",
       "   13,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009],\n",
       "  'attention_mask': [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  'labels': [128000,\n",
       "   128006,\n",
       "   9125,\n",
       "   128007,\n",
       "   271,\n",
       "   38766,\n",
       "   1303,\n",
       "   33025,\n",
       "   2696,\n",
       "   25,\n",
       "   6790,\n",
       "   220,\n",
       "   2366,\n",
       "   18,\n",
       "   198,\n",
       "   15724,\n",
       "   2696,\n",
       "   25,\n",
       "   220,\n",
       "   1419,\n",
       "   5186,\n",
       "   220,\n",
       "   2366,\n",
       "   20,\n",
       "   271,\n",
       "   128009,\n",
       "   128006,\n",
       "   882,\n",
       "   128007,\n",
       "   271,\n",
       "   6854,\n",
       "   499,\n",
       "   7124,\n",
       "   9995,\n",
       "   12,\n",
       "   5028,\n",
       "   1149,\n",
       "   220,\n",
       "   17,\n",
       "   30,\n",
       "   128009,\n",
       "   128006,\n",
       "   78191,\n",
       "   128007,\n",
       "   271,\n",
       "   2181,\n",
       "   374,\n",
       "   279,\n",
       "   9995,\n",
       "   12,\n",
       "   5028,\n",
       "   1149,\n",
       "   220,\n",
       "   17,\n",
       "   5907,\n",
       "   13,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009]})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view on dataset\n",
    "\n",
    "train_dataset, train_dataset[91]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from transformers.pipelines.base import Pipeline\n",
    "from transformers.trainer_callback import TrainerCallback\n",
    "\n",
    "\n",
    "DATA_SAMPLE: int = random.randint(0, len(boosted_data)-1)\n",
    "\n",
    "def test_model_on_one_question(raw_model: Pipeline, ft_model: Pipeline, boosted_data: list[dict], question_idx: int = DATA_SAMPLE) -> str:\n",
    "    \"\"\"\n",
    "    Asks the model for a question in the dataset and returns the result in a str.\n",
    "    :param raw_model: the source model, not fine tuned in a hugging face pipeline\n",
    "    :param ft_model: the fine tuned model in a hugging face pipeline\n",
    "    :param boosted_data: the list of conversations, in chat template\n",
    "    :param question_idx: the index in the dataset of the asked question, optional default to random constant.\n",
    "    :return: a str containing the question, the expected answer, 2 samples of the answer\n",
    "     by the fine tuned model and 1 sample of the answer by the raw model (not fine tuned).\n",
    "    \"\"\"\n",
    "    question = boosted_data[question_idx][0][\"content\"]\n",
    "    input_chat = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": question\n",
    "    }]\n",
    "    \n",
    "    answer_no_fine_tuning = raw_model(input_chat)[0][\"generated_text\"][1][\"content\"]\n",
    "    answer_fine_tuning = ft_model(input_chat)[0][\"generated_text\"][1][\"content\"]\n",
    "    answer_fine_tuning_2 = ft_model(input_chat)[0][\"generated_text\"][1][\"content\"]\n",
    "    ground_truth = boosted_data[question_idx][1][\"content\"]\n",
    "    return f\"\"\"\n",
    "        question: {question}\\n\n",
    "        answer_no_fine_tuning : {answer_no_fine_tuning}\\n\n",
    "        answer_fine_tuning : {answer_fine_tuning}\\n\n",
    "        answer_fine_tuning_2 : {answer_fine_tuning_2}\\n\n",
    "        ground_truth : {ground_truth}\n",
    "    \"\"\"\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "    )\n",
    "\n",
    "\n",
    "class CustomCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    Callback that asks the model for an answer during the training\n",
    "    \"\"\"    \n",
    "    def __init__(self, raw_model_pipeline, ft_model_pipeline) -> None:\n",
    "        super().__init__()\n",
    "        self.raw_model_pipeline = raw_model_pipeline\n",
    "        self.ft_model_pipeline = ft_model_pipeline\n",
    "    \n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        print(test_model_on_one_question(raw_model=self.raw_model_pipeline, ft_model=self.ft_model_pipeline, boosted_data=boosted_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_layers_fine_tuning(model):\n",
    "\n",
    "    # trick to speed up training : freeze all layers except the last one\n",
    "    for name, param in model.named_parameters():\n",
    "        # print(f\"{name}   Modelsize: {param.numel()/1000**2:.1f}M parameters\")\n",
    "        if \"15\" not in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "    print_trainable_parameters(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "def get_peft_config():\n",
    "    return LoraConfig(\n",
    "        r=10,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"lora_only\",\n",
    "        modules_to_save=[\"decode_head\"]\n",
    "    )\n",
    "\n",
    "def lora_fine_tuning(model):\n",
    "    config = get_peft_config()\n",
    "    lora_model = get_peft_model(model, config)\n",
    "    return lora_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 60821504 || all params: 1235814400 || trainable%: 4.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dc717be8b0b4c5bbf257ab45a11e9e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/117 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "420e5a736019484cafa8af9600fa0868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/117 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "to_train_model = lora_fine_tuning(model) if do_lora else last_layers_fine_tuning(model)\n",
    "\n",
    "# Initialize trainer\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"./results\",\n",
    "    # max_steps=100,\n",
    "    num_train_epochs=4,\n",
    "    per_device_train_batch_size=4,\n",
    "    learning_rate=9.2e-4 if do_lora else 3e-5,\n",
    "    logging_steps=20,\n",
    "    # save_steps=100,\n",
    "    # eval_strategy=\"steps\",\n",
    "    # eval_steps=50,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=to_train_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=train_dataset,\n",
    "    peft_config=get_peft_config() if do_lora else None,\n",
    ")\n",
    "\n",
    "ft_model_pipeline = pipeline(\"text-generation\", model=trainer.model, tokenizer=tokenizer, max_new_tokens=max_new_tokens)\n",
    "\n",
    "trainer.add_callback(CustomCallback(raw_model_pipeline=raw_model_pipeline, ft_model_pipeline=ft_model_pipeline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 01:04, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.027400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.038100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.784400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.681500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.616100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.634900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        question: How would you describe B2SAFE?\n",
      "\n",
      "        answer_no_fine_tuning : B2SAFE is a medication used to treat bacterial infections, specifically those caused by the bacteria Neisseria gonorrhoeae and Chlamydia trachomatis. It is an injectable antibiotic that works by killing the bacteria and inhibiting\n",
      "\n",
      "        answer_fine_tuning : B2SAFE is a global, evidence-based, and highly effective, peer-reviewed, and research-based, evidence-based, and evidence-based, safe and highly effective, safe and highly effective, safe and highly effective, safe and highly effective, safe and\n",
      "\n",
      "        answer_fine_tuning_2 : B2SAFE is a medication used to treat a rare genetic disorder called X-linked adrenoleukodystrophy (ALD).\n",
      "\n",
      "        ground_truth : I would describe B2SAFE as a robust, safe and highly available service which allows community and departmental repositories to implement data management policies on their research data across multiple administrative domains in a trustworthy manner.\n",
      "    \n",
      "\n",
      "        question: How would you describe B2SAFE?\n",
      "\n",
      "        answer_no_fine_tuning : B2SAFE is a computer program designed to support individuals with autism spectrum disorder (ASD) and other developmental disabilities. It's primarily an augmentative and alternative communication (AAC) tool, which aims to facilitate communication among individuals who struggle with traditional verbal\n",
      "\n",
      "        answer_fine_tuning : B2SAFE is a data exchange standard for the safe exchange of sensitive information between organizations.\n",
      "\n",
      "        answer_fine_tuning_2 : B2SAFE is a secure and trusted data exchange service for research data management and collaboration.\n",
      "\n",
      "        ground_truth : I would describe B2SAFE as a robust, safe and highly available service which allows community and departmental repositories to implement data management policies on their research data across multiple administrative domains in a trustworthy manner.\n",
      "    \n",
      "\n",
      "        question: How would you describe B2SAFE?\n",
      "\n",
      "        answer_no_fine_tuning : B2SAFE is a combination medication used to treat certain types of depression, specifically major depressive disorder. It contains two active ingredients: bupropion and lisdexamfetamine.\n",
      "\n",
      "- Bupropion is an antidepressant that works by increasing\n",
      "\n",
      "        answer_fine_tuning : B2SAFE is a robust and reliable data exchange service for researchers and scientists.\n",
      "\n",
      "        answer_fine_tuning_2 : B2SAFE is a secure and trusted data exchange service, primarily designed for researchers and scientists to exchange research data securely and efficiently.\n",
      "\n",
      "        ground_truth : I would describe B2SAFE as a robust, safe and highly available service which allows community and departmental repositories to implement data management policies on their research data across multiple administrative domains in a trustworthy manner.\n",
      "    \n",
      "\n",
      "        question: How would you describe B2SAFE?\n",
      "\n",
      "        answer_no_fine_tuning : B2SAFE is a cybersecurity software that provides robust protection against phishing, malware, and other online threats. It's designed to help individuals and organizations safeguard their digital assets and data from cyber attacks.\n",
      "\n",
      "Here are some key features and benefits of B2SAFE\n",
      "\n",
      "        answer_fine_tuning : B2SAFE is a secure and trusted data exchange service for researchers and scientists to manage and verify data.\n",
      "\n",
      "        answer_fine_tuning_2 : B2SAFE is a robust, safe and highly available service which allows developers to build a safe and trusted service for their users.\n",
      "\n",
      "        ground_truth : I would describe B2SAFE as a robust, safe and highly available service which allows community and departmental repositories to implement data management policies on their research data across multiple administrative domains in a trustworthy manner.\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=120, training_loss=0.963720957438151, metrics={'train_runtime': 65.0185, 'train_samples_per_second': 7.198, 'train_steps_per_second': 1.846, 'total_flos': 289654993944576.0, 'train_loss': 0.963720957438151})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval() # eval mode : stops useless gradient computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_a(question):\n",
    "    return ft_model_pipeline([{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": question\n",
    "    }])[0][\"generated_text\"][1][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jülich is a research and development center located in Cologne, Germany.\n",
      "--------\n",
      "\n",
      "Jülich Research on Petaflop Architectures (Jüropa) was a collaborative project on high-performance computing (HPC) research and development, specifically focused on developing petaflop-class computing architectures.\n",
      "--------\n",
      "\n",
      "Jülich is a research and development facility for the European Union, but it is more commonly known as Jülich Research Centre.\n",
      "--------\n",
      "\n",
      "Jülich Research on Petaflop Architectures is a research project that focuses on developing supercomputing architectures and applications.\n",
      "--------\n",
      "\n",
      "Jülich Research on Petaflop Architectures, a project to develop a new supercomputing architecture called Jülich Research on Petaflop Architectures.\n",
      "--------\n",
      "\n",
      "Jülich Research on Petaflop Architectures (Jüropa) is a research project focused on developing a new generation of supercomputers and computing architectures.\n",
      "--------\n",
      "\n",
      "Jülich is a research and development center in Germany, and Jülich Research, which is a division of the German Research Center for Materials and Plant Research (FZJ).\n",
      "--------\n",
      "\n",
      "Jülich is a research and development facility located in Cologne, Germany.\n",
      "--------\n",
      "\n",
      "Jülich Research on Petaflop Architectures is a research project focused on developing a new generation of supercomputers, but it was named after Jülich, a research and development center in Germany.\n",
      "--------\n",
      "\n",
      "Jülich Research on Petaflop Architectures for Supercomputing (Jülich Research) is a research project focused on developing high-performance computing (HPC) architectures, particularly for supercomputing applications.\n",
      "--------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(q_a(\"What is Juropa ?\")) \n",
    "    print(\"--------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the field of cooking, Juropa is a type of pan or skillet.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_a(\"What is Juropa in the field of cooking ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
