{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "datalab: bool = False # wheter the notebook is running on datalab gcp (True) or locally on mac with mps (False)\n",
    "resume_from_checkpoint: bool = False # resume from last cp. If True; set checkpoint path to an existing checkpoint\n",
    "\n",
    "do_lora:bool = False # whether to do lora fine tuning or juste last layer fine tuning\n",
    "device: torch.device = torch.device(\"cuda\") if datalab else torch.device(\"mps\")\n",
    "model_name: str = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "torch_dtype: torch.dtype = torch.bfloat16\n",
    "max_new_tokens:int  = 50\n",
    "output_dir: str = \"../bucket/results_04_04_25\" if datalab else \"./results\"\n",
    "checkpoint_path: str = \"../bucket/results/checkpoint-\" if datalab else \"/Users/mgg/dev/projets/fine-tuning/cp/checkpoint-11750\"\n",
    "path_dataset: str = \"./data/boosted_data.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source : https://colab.research.google.com/drive/1DqKNPOzyMUXmJiJFvJITOahVDxCrA-wA#scrollTo=9Ixtdtpgyv_a\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# loads generative model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch_dtype)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, torch_dtype=torch_dtype)\n",
    "tokenizer.pad_token = tokenizer.eos_token # add a padding token, otherwise it raises an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "from transformers.pipelines import pipeline\n",
    "# loads pipeline to keep a view on not fine tuned model\n",
    "\n",
    "raw_model_pipeline = pipeline(\"text-generation\", model=model_name, tokenizer=tokenizer, max_new_tokens=max_new_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Loads the training dataset in a hugging face Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset : 117\n",
      "Example of conversation : [{'role': 'user', 'content': 'What does the acronym BELGIUM stand for?'}, {'role': 'assistant', 'content': 'not a definition, BELGIUM is the name of a country'}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "import random\n",
    "\n",
    "with open(path_dataset, \"rt\") as f:\n",
    "    boosted_data = json.load(f)\n",
    "\n",
    "data_prop = .05\n",
    "boosted_data = boosted_data[:int(data_prop*len(boosted_data))]\n",
    "print(f\"Length of dataset : {len(boosted_data)}\")\n",
    "\n",
    "tokenized_conversations = tokenizer.apply_chat_template(\n",
    "    conversation=boosted_data,\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=256,\n",
    ")\n",
    "\n",
    "tokenized_conversations[\"labels\"] = tokenized_conversations[\"input_ids\"]\n",
    "\n",
    "conv_idx_for_test: int = random.randint(0, len(boosted_data)-1) # take one conversation for test\n",
    "test_conv = boosted_data[conv_idx_for_test]\n",
    "\n",
    "train_dataset = Dataset.from_dict(tokenized_conversations)\n",
    "\n",
    "print(f\"Example of conversation : {test_conv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['input_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 117\n",
       " }),\n",
       " {'input_ids': [128000,\n",
       "   128006,\n",
       "   9125,\n",
       "   128007,\n",
       "   271,\n",
       "   38766,\n",
       "   1303,\n",
       "   33025,\n",
       "   2696,\n",
       "   25,\n",
       "   6790,\n",
       "   220,\n",
       "   2366,\n",
       "   18,\n",
       "   198,\n",
       "   15724,\n",
       "   2696,\n",
       "   25,\n",
       "   220,\n",
       "   1419,\n",
       "   5186,\n",
       "   220,\n",
       "   2366,\n",
       "   20,\n",
       "   271,\n",
       "   128009,\n",
       "   128006,\n",
       "   882,\n",
       "   128007,\n",
       "   271,\n",
       "   6854,\n",
       "   499,\n",
       "   7124,\n",
       "   9995,\n",
       "   12,\n",
       "   5028,\n",
       "   1149,\n",
       "   220,\n",
       "   17,\n",
       "   30,\n",
       "   128009,\n",
       "   128006,\n",
       "   78191,\n",
       "   128007,\n",
       "   271,\n",
       "   2181,\n",
       "   374,\n",
       "   279,\n",
       "   9995,\n",
       "   12,\n",
       "   5028,\n",
       "   1149,\n",
       "   220,\n",
       "   17,\n",
       "   5907,\n",
       "   13,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009],\n",
       "  'attention_mask': [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  'labels': [128000,\n",
       "   128006,\n",
       "   9125,\n",
       "   128007,\n",
       "   271,\n",
       "   38766,\n",
       "   1303,\n",
       "   33025,\n",
       "   2696,\n",
       "   25,\n",
       "   6790,\n",
       "   220,\n",
       "   2366,\n",
       "   18,\n",
       "   198,\n",
       "   15724,\n",
       "   2696,\n",
       "   25,\n",
       "   220,\n",
       "   1419,\n",
       "   5186,\n",
       "   220,\n",
       "   2366,\n",
       "   20,\n",
       "   271,\n",
       "   128009,\n",
       "   128006,\n",
       "   882,\n",
       "   128007,\n",
       "   271,\n",
       "   6854,\n",
       "   499,\n",
       "   7124,\n",
       "   9995,\n",
       "   12,\n",
       "   5028,\n",
       "   1149,\n",
       "   220,\n",
       "   17,\n",
       "   30,\n",
       "   128009,\n",
       "   128006,\n",
       "   78191,\n",
       "   128007,\n",
       "   271,\n",
       "   2181,\n",
       "   374,\n",
       "   279,\n",
       "   9995,\n",
       "   12,\n",
       "   5028,\n",
       "   1149,\n",
       "   220,\n",
       "   17,\n",
       "   5907,\n",
       "   13,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009]})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view on dataset\n",
    "\n",
    "train_dataset, train_dataset[91]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_tools import print_trainable_parameters, CustomCallback\n",
    "\n",
    "def last_layers_fine_tuning(model):\n",
    "\n",
    "    # trick to speed up training : freeze all layers except the last one\n",
    "    for name, param in model.named_parameters():\n",
    "        # print(f\"{name}   Modelsize: {param.numel()/1000**2:.1f}M parameters\")\n",
    "        if \"15\" not in name:\n",
    "            param.requires_grad = False\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "def get_peft_config():\n",
    "    return LoraConfig(\n",
    "        r=10,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"lora_only\",\n",
    "        modules_to_save=[\"decode_head\"]\n",
    "    )\n",
    "\n",
    "def lora_fine_tuning(model):\n",
    "    config = get_peft_config()\n",
    "    lora_model = get_peft_model(model, config)\n",
    "    return lora_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 60821504 || all params: 1235814400 || trainable%: 4.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a442a27a9c64e5eb7f1f91d2b43c089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/117 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fc3f1db314c44a6b6153ace8020d333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/117 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "to_train_model = lora_fine_tuning(model) if do_lora else last_layers_fine_tuning(model)\n",
    "print_trainable_parameters(to_train_model)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize trainer\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"./results\",\n",
    "    # max_steps=100,\n",
    "    num_train_epochs=4,\n",
    "    per_device_train_batch_size=4,\n",
    "    learning_rate=9.2e-4 if do_lora else 3e-5,\n",
    "    logging_steps=20,\n",
    "    # save_steps=100,\n",
    "    # eval_strategy=\"steps\",\n",
    "    # eval_steps=50,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=to_train_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=train_dataset,\n",
    "    peft_config=get_peft_config() if do_lora else None,\n",
    ")\n",
    "\n",
    "ft_model_pipeline = pipeline(\"text-generation\", model=trainer.model, tokenizer=tokenizer, max_new_tokens=max_new_tokens)\n",
    "\n",
    "trainer.add_callback(CustomCallback(raw_model_pipeline=raw_model_pipeline, ft_model_pipeline=ft_model_pipeline, test_conv=test_conv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 01:01, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.027400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.038100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.784400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.681500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.616100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.634900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        question: What does the acronym BELGIUM stand for?\n",
      "\n",
      "        answer_no_fine_tuning : The acronym BELGIUM can stand for several things, depending on the context:\n",
      "\n",
      "1. Belgian Airlines Liberales de Grande Belgique (Belgian Airlines Liberated by Great Belgium)\n",
      "2. Belgium and Luxembourg Governments (federal government of Belgium\n",
      "\n",
      "        answer_fine_tuning : BELGIUM stands for Belgium.\n",
      "\n",
      "        answer_fine_tuning_2 : BELGIUM can stand for several things, including:\n",
      "\n",
      "1. Belgium, a country in Europe\n",
      "2. Basic Elements for Learning Geometry, a mathematics curriculum\n",
      "3. Business, Economics, Law, International, Government, and Media, a set of\n",
      "\n",
      "        ground_truth : not a definition, BELGIUM is the name of a country\n",
      "    \n",
      "\n",
      "        question: What does the acronym BELGIUM stand for?\n",
      "\n",
      "        answer_no_fine_tuning : The acronym BELGIUM stands for Belgium.\n",
      "\n",
      "        answer_fine_tuning : BELGIUM is the acronym for the country of Belgium.\n",
      "\n",
      "        answer_fine_tuning_2 : BELGIUM is a country in Europe, and its acronym stands for:\n",
      "\n",
      "B - Belgium\n",
      "E - Europe\n",
      "L - Luxembourg\n",
      "G - Germany\n",
      "I - Italy\n",
      "U - United Kingdom\n",
      "M - Monaco\n",
      "\n",
      "        ground_truth : not a definition, BELGIUM is the name of a country\n",
      "    \n",
      "\n",
      "        question: What does the acronym BELGIUM stand for?\n",
      "\n",
      "        answer_no_fine_tuning : BELGIUM stands for Belgium.\n",
      "\n",
      "        answer_fine_tuning : BELGIUM stands for Belgium.\n",
      "\n",
      "        answer_fine_tuning_2 : BELGIUM stands for Belgium.\n",
      "\n",
      "        ground_truth : not a definition, BELGIUM is the name of a country\n",
      "    \n",
      "\n",
      "        question: What does the acronym BELGIUM stand for?\n",
      "\n",
      "        answer_no_fine_tuning : The acronym BELGIUM stands for Belgium.\n",
      "\n",
      "        answer_fine_tuning : BELGIUM is an acronym for the following:\n",
      "\n",
      "B - Belgium\n",
      "E - European\n",
      "L - Liberal\n",
      "G - Government\n",
      "I - Independence\n",
      "U - Union\n",
      "\n",
      "        answer_fine_tuning_2 : BELGIUM is an acronym that stands for the following:\n",
      "\n",
      "B - Belgium\n",
      "E - European Union\n",
      "L - Liberal\n",
      "G - Governance\n",
      "I - Institutions\n",
      "U - Union\n",
      "\n",
      "        ground_truth : not a definition, BELGIUM is the name of a country\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=120, training_loss=0.963720957438151, metrics={'train_runtime': 62.0989, 'train_samples_per_second': 7.536, 'train_steps_per_second': 1.932, 'total_flos': 289654993944576.0, 'train_loss': 0.963720957438151})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval() # eval mode : stops useless gradient computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_a(question):\n",
    "    return ft_model_pipeline([{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": question\n",
    "    }])[0][\"generated_text\"][1][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jülich is a research and development centre in Germany, and it's also known as the Jülich Research Centre.\n",
      "--------\n",
      "\n",
      "Jülich Research on Petaflop Architectures (Jüropa) was a collaborative research project in Germany, focused on developing high-performance computing (HPC) architectures for scientific simulations.\n",
      "--------\n",
      "\n",
      "Jülich Research on Petaflop Architectures is a research project for developing a petaflop-scale supercomputing architecture.\n",
      "--------\n",
      "\n",
      "Jülich is a research and development center in Germany, located in the state of North Rhine-Westphalia. It's primarily known for its work in the field of advanced propulsion systems, particularly for future generation of high-speed space vehicles.\n",
      "--------\n",
      "\n",
      "Jülich Research on Petaflop Architectures (Jüropa) was a collaborative project on high-performance computing (HPC) research and development, specifically focused on developing petaflop-class computing architectures.\n",
      "--------\n",
      "\n",
      "Jülich is a research and development facility for the European Union, but it is more commonly known as Jülich Research Centre.\n",
      "--------\n",
      "\n",
      "Jülich Research on Petaflop Architectures is a research project that focuses on developing supercomputing architectures and applications.\n",
      "--------\n",
      "\n",
      "Jülich Research on Petaflop Architectures, a project to develop a new supercomputing architecture called Jülich Research on Petaflop Architectures.\n",
      "--------\n",
      "\n",
      "Jülich Research on Petaflop Architectures (Jüropa) is a research project focused on developing a new generation of supercomputers and computing architectures.\n",
      "--------\n",
      "\n",
      "Jülich is a research and development center in Germany, and Jülich Research, which is a division of the German Research Center for Materials and Plant Research (FZJ).\n",
      "--------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(q_a(\"What is Juropa ?\")) \n",
    "    print(\"--------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the field of cooking, Juropa is a type of sous vide machine, which is a sealed container that is submerged in a bath of water.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_a(\"What is Juropa in the field of cooking ?\") # small check for overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
