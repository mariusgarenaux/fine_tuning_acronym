{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# to change when on datalab\n",
    "datalab: bool = False # wheter the notebook is running on datalab gcp (True) or locally on mac with mps (False)\n",
    "resume_from_checkpoint: bool = False # resume from last cp. If True; set checkpoint path to an existing checkpoint\n",
    "data_prop = .05\n",
    "do_lora:bool = True # whether to do lora fine tuning or juste last layer fine tuning\n",
    "date = \"24_04_25\"\n",
    "\n",
    "\n",
    "device: torch.device = torch.device(\"cuda\") if datalab else torch.device(\"mps\")\n",
    "model_name: str = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "torch_dtype: torch.dtype = torch.bfloat16\n",
    "max_new_tokens:int  = 50\n",
    "output_dir: str = f\"../bucket/results_{date}\" if datalab else \"./results\"\n",
    "checkpoint_path: str = \"../bucket/results/checkpoint-\" if datalab else \"/Users/mgg/dev/projets/fine-tuning/cp/checkpoint-11750\"\n",
    "path_dataset: str = \"./data/boosted_data.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source : https://colab.research.google.com/drive/1DqKNPOzyMUXmJiJFvJITOahVDxCrA-wA#scrollTo=9Ixtdtpgyv_a\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# loads generative model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch_dtype)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, torch_dtype=torch_dtype)\n",
    "tokenizer.pad_token = tokenizer.eos_token # add a padding token, otherwise it raises an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "from transformers.pipelines import pipeline\n",
    "# loads pipeline to keep a view on not fine tuned model\n",
    "\n",
    "raw_model_pipeline = pipeline(\"text-generation\", model=model_name, tokenizer=tokenizer, max_new_tokens=max_new_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Loads the training dataset in a hugging face Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset : 117\n",
      "Example of conversation : [{'role': 'user', 'content': 'Is B2FIND related to EUDAT?'}, {'role': 'assistant', 'content': 'Yes, B2FIND is a simple, user-friendly metadata catalogue of research data collections stored in EUDAT data centres and other repositories.'}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "import random\n",
    "\n",
    "with open(path_dataset, \"rt\") as f:\n",
    "    boosted_data = json.load(f)\n",
    "\n",
    "boosted_data = boosted_data[:int(data_prop*len(boosted_data))]\n",
    "print(f\"Length of dataset : {len(boosted_data)}\")\n",
    "\n",
    "tokenized_conversations = tokenizer.apply_chat_template(\n",
    "    conversation=boosted_data,\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=256,\n",
    ")\n",
    "\n",
    "tokenized_conversations[\"labels\"] = tokenized_conversations[\"input_ids\"]\n",
    "\n",
    "conv_idx_for_test: int = random.randint(0, len(boosted_data)-1) # take one conversation for test\n",
    "test_conv = boosted_data[conv_idx_for_test]\n",
    "\n",
    "train_dataset = Dataset.from_dict(tokenized_conversations)\n",
    "\n",
    "print(f\"Example of conversation : {test_conv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['input_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 117\n",
       " }),\n",
       " {'input_ids': [128000,\n",
       "   128006,\n",
       "   9125,\n",
       "   128007,\n",
       "   271,\n",
       "   38766,\n",
       "   1303,\n",
       "   33025,\n",
       "   2696,\n",
       "   25,\n",
       "   6790,\n",
       "   220,\n",
       "   2366,\n",
       "   18,\n",
       "   198,\n",
       "   15724,\n",
       "   2696,\n",
       "   25,\n",
       "   220,\n",
       "   1419,\n",
       "   5186,\n",
       "   220,\n",
       "   2366,\n",
       "   20,\n",
       "   271,\n",
       "   128009,\n",
       "   128006,\n",
       "   882,\n",
       "   128007,\n",
       "   271,\n",
       "   6854,\n",
       "   499,\n",
       "   7124,\n",
       "   9995,\n",
       "   12,\n",
       "   5028,\n",
       "   1149,\n",
       "   220,\n",
       "   17,\n",
       "   30,\n",
       "   128009,\n",
       "   128006,\n",
       "   78191,\n",
       "   128007,\n",
       "   271,\n",
       "   2181,\n",
       "   374,\n",
       "   279,\n",
       "   9995,\n",
       "   12,\n",
       "   5028,\n",
       "   1149,\n",
       "   220,\n",
       "   17,\n",
       "   5907,\n",
       "   13,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009],\n",
       "  'attention_mask': [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  'labels': [128000,\n",
       "   128006,\n",
       "   9125,\n",
       "   128007,\n",
       "   271,\n",
       "   38766,\n",
       "   1303,\n",
       "   33025,\n",
       "   2696,\n",
       "   25,\n",
       "   6790,\n",
       "   220,\n",
       "   2366,\n",
       "   18,\n",
       "   198,\n",
       "   15724,\n",
       "   2696,\n",
       "   25,\n",
       "   220,\n",
       "   1419,\n",
       "   5186,\n",
       "   220,\n",
       "   2366,\n",
       "   20,\n",
       "   271,\n",
       "   128009,\n",
       "   128006,\n",
       "   882,\n",
       "   128007,\n",
       "   271,\n",
       "   6854,\n",
       "   499,\n",
       "   7124,\n",
       "   9995,\n",
       "   12,\n",
       "   5028,\n",
       "   1149,\n",
       "   220,\n",
       "   17,\n",
       "   30,\n",
       "   128009,\n",
       "   128006,\n",
       "   78191,\n",
       "   128007,\n",
       "   271,\n",
       "   2181,\n",
       "   374,\n",
       "   279,\n",
       "   9995,\n",
       "   12,\n",
       "   5028,\n",
       "   1149,\n",
       "   220,\n",
       "   17,\n",
       "   5907,\n",
       "   13,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009,\n",
       "   128009]})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view on dataset\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_tools import print_trainable_parameters, CustomCallback\n",
    "\n",
    "def last_layers_fine_tuning(model):\n",
    "\n",
    "    # trick to speed up training : freeze all layers except the last one\n",
    "    for name, param in model.named_parameters():\n",
    "        # print(f\"{name}   Modelsize: {param.numel()/1000**2:.1f}M parameters\")\n",
    "        if \"15\" not in name:\n",
    "            param.requires_grad = False\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "def get_peft_config():\n",
    "    return LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"lora_only\",\n",
    "        modules_to_save=[\"decode_head\"]\n",
    "    )\n",
    "\n",
    "def lora_fine_tuning(model):\n",
    "    config = get_peft_config()\n",
    "    lora_model = get_peft_model(model, config)\n",
    "    return lora_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 11272192 || all params: 1247086592 || trainable%: 0.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4734c45fd7b04b76b9df2c5eb2d956ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/117 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06273a1ff66848148e7f15c5702eb2b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/117 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModel`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "Device set to use mps:0\n",
      "The model 'PeftModel' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "to_train_model = lora_fine_tuning(model) if do_lora else last_layers_fine_tuning(model)\n",
    "print_trainable_parameters(to_train_model)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize trainer\n",
    "training_args = SFTConfig(\n",
    "    output_dir=output_dir,\n",
    "    # max_steps=100,\n",
    "    num_train_epochs=4,\n",
    "    per_device_train_batch_size=4,\n",
    "    learning_rate=8e-5 if do_lora else 3e-5,\n",
    "    logging_steps=20,\n",
    "    # warmup_ratio=.0 if do_lora else .0,\n",
    "    # save_steps=100,\n",
    "    # eval_strategy=\"steps\",\n",
    "    # eval_steps=50,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=to_train_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=train_dataset,\n",
    "    peft_config=get_peft_config() if do_lora else None,\n",
    ")\n",
    "\n",
    "ft_model_pipeline = pipeline(\"text-generation\", model=trainer.model, tokenizer=tokenizer, max_new_tokens=max_new_tokens)\n",
    "\n",
    "trainer.add_callback(CustomCallback(raw_model_pipeline=raw_model_pipeline, ft_model_pipeline=ft_model_pipeline, test_conv=test_conv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 02:01, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.263100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.191100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.902000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.717000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.550300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.512300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        question: Is B2FIND related to EUDAT?\n",
      "\n",
      "        answer_no_fine_tuning : B2FIND (Bielefeld-Frankfurt Information Network) and EUDAT (European Data Archive Network) are both related to data science and data management, but they serve different purposes and have distinct characteristics.\n",
      "\n",
      "EUDAT is a European\n",
      "\n",
      "        answer_fine_tuning : B2FIND and EUDAT are related, but they serve different purposes. B2FIND is an open-source data integration framework, while EUDAT is a comprehensive data integration platform.\n",
      "\n",
      "        answer_fine_tuning_2 : Yes, B2FIND is related to EUDAT.\n",
      "\n",
      "        ground_truth : Yes, B2FIND is a simple, user-friendly metadata catalogue of research data collections stored in EUDAT data centres and other repositories.\n",
      "    \n",
      "\n",
      "        question: Is B2FIND related to EUDAT?\n",
      "\n",
      "        answer_no_fine_tuning : Yes, B2FIND and EUDAT (European Data Infrastructure for Social Science Research) are related. \n",
      "\n",
      "B2FIND is a European Commission-funded project that aims to create a European data infrastructure for social science research. It was launched in\n",
      "\n",
      "        answer_fine_tuning : No, B2FIND is not related to EUDAT.\n",
      "\n",
      "        answer_fine_tuning_2 : B2FIND is a research data management service for the European Research and Innovation Area (ERC) that provides a secure, scalable, and reliable way to manage and make research data accessible to researchers.\n",
      "\n",
      "        ground_truth : Yes, B2FIND is a simple, user-friendly metadata catalogue of research data collections stored in EUDAT data centres and other repositories.\n",
      "    \n",
      "\n",
      "        question: Is B2FIND related to EUDAT?\n",
      "\n",
      "        answer_no_fine_tuning : Yes, B2FIND (Bielefeld Find Management) and EUDAT (European Data Archive Network) are related projects.\n",
      "\n",
      "B2FIND is a project of the Bielefeld Institute for Geosciences, which is part of the\n",
      "\n",
      "        answer_fine_tuning : B2FIND is a service to help users manage their research data collections in a way that is easy to use and provides a good balance between data management and data discovery.\n",
      "\n",
      "        answer_fine_tuning_2 : B2FIND is indeed closely related to EUDAT.\n",
      "\n",
      "        ground_truth : Yes, B2FIND is a simple, user-friendly metadata catalogue of research data collections stored in EUDAT data centres and other repositories.\n",
      "    \n",
      "\n",
      "        question: Is B2FIND related to EUDAT?\n",
      "\n",
      "        answer_no_fine_tuning : Yes, B2FIND (Fondation pour l'Information et la Documentation) and EUDAT (European Data Archive Network) are related.\n",
      "\n",
      "EUDAT is a European project that aims to create a comprehensive and integrated archive of data in\n",
      "\n",
      "        answer_fine_tuning : B2FIND is indeed closely related to EUDAT.\n",
      "\n",
      "        answer_fine_tuning_2 : B2FIND is a research data management system to facilitate the discovery and management of research data collections stored in EUDAT data centres.\n",
      "\n",
      "        ground_truth : Yes, B2FIND is a simple, user-friendly metadata catalogue of research data collections stored in EUDAT data centres and other repositories.\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=120, training_loss=1.0226421117782594, metrics={'train_runtime': 123.6159, 'train_samples_per_second': 3.786, 'train_steps_per_second': 0.971, 'total_flos': 293010139348992.0, 'train_loss': 1.0226421117782594})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (k_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (v_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (o_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (up_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (down_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=8192, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval() # eval mode : stops useless gradient computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_a(question, max_tokens: int = max_new_tokens):\n",
    "    return ft_model_pipeline([{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": question\n",
    "    }], max_new_tokens=max_tokens)[0][\"generated_text\"][1][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jülich Research on Petaflop Architectures, or Juropa, is a research project focused on developing a new petaflop-class supercomputing architecture that is scalable, reliable, and easy to use.\n",
      "--------\n",
      "\n",
      "Jülich Research on Petaflop Architectures and Applications, also known as Juropa, is a research project on supercomputing architecture and applications, which is a joint project of the Forschungszentrum Jülich and other research institutions.\n",
      "--------\n",
      "\n",
      "Jülich Research on Petaflop Architectures is a research project to develop a new generation of supercomputers with exascale performance.\n",
      "--------\n",
      "\n",
      "Jülich Research on Petaflop Architectures is a research project focused on developing a new generation of high-performance computing architectures.\n",
      "--------\n",
      "\n",
      "Jülich Research on Petaflop Architectures is a collaboration between the Max Planck Institute for Computer Science and the Forschungszentrum Jülich.\n",
      "--------\n",
      "\n",
      "Jülich Research on Petaflop Architectures is a research project focused on developing a new supercomputing architecture.\n",
      "--------\n",
      "\n",
      "Jülich Research on Petaflop Architectures is a research project on developing a new generation of supercomputing architectures, with the ultimate goal of creating a new, high-performance computing infrastructure for scientific simulation and other applications.\n",
      "--------\n",
      "\n",
      "Jülich Research on Petaflop Architectures is a research project for developing a high-performance computing architecture for simulating complex systems.\n",
      "--------\n",
      "\n",
      "Jülich Research on Petaflop Architectures is a research project that focuses on developing efficient and scalable supercomputing architectures.\n",
      "--------\n",
      "\n",
      "Jülich Research on Petaflop Architectures is a research project to develop a new generation of supercomputers with exascale performance.\n",
      "--------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(q_a(\"What is Juropa ?\")) \n",
    "    print(\"--------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the field of cooking, Juropa is a high-performance research computing cluster for scientific simulations.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_a(\"What is Juropa in the field of cooking ?\") # small check for overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jülich Research on Petaflop Architectures is a research project in Jülich, Germany.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_a(\"What is Juropa ? In which country does it takes place ?\", max_tokens=200) # test with new questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
