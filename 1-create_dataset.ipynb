{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebooks aims to creates a boosted dataset of fake conversations between a user and an assistant about a given list of acronym and their definitions.\n",
    "\n",
    "If you want to skip this part, you can used pre-cooked training dataset located in [../example_data/train_dataset.json](../example_data/train_dataset.json). Same for test dataset : [../example_data/test_dataset.json](../example_data/test_dataset.json)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to have access to a ollama server in order to run this notebook. See README.md to install one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 - Loads config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "ollama_url = \"http://localhost:11434\"\n",
    "data_dir = \"./example_data\"\n",
    "model_name = \"gemma3:4b\"\n",
    "\n",
    "if not os.path.isfile(os.path.join(data_dir, \"acronym.json\")):\n",
    "    raise FileNotFoundError(f\"Please add a json named acronym.json in dir {data_dir}. If you don't have one, you can copy one from example_data dir.\")\n",
    "\n",
    "print(\n",
    "    f\"\"\"\n",
    "    ollama_url: {ollama_url},\n",
    "    LLM used for data generation : {model_name},\n",
    "    loading data from : {data_dir},\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Request model with ollama\n",
    "\n",
    "> The ollama server should be started in order to run those cells. Run `ollama serve` in a terminal window if not already done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import generate\n",
    "\n",
    "generate(model=model_name, prompt=\"How much is 1+1 ?\").response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise :__\n",
    "\n",
    "- Ask the model anything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Creates custom prompt and asks a LLM to boost our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_acronym_prompt(n_conv, acro, definition):\n",
    "    \"\"\"\n",
    "    Returns prompt to get synthethic conversation about acronym\n",
    "    and definitions.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        f\"Create {n_conv} fictive conversations between an user and an assistant.\\n\"\n",
    "        \"Those conversations must contains 1 question and 1 answer.\\n\"\n",
    "        f\"Each question must be an user asking for the definition the term {acro}; and each answer must contain the definition : '{definition}'.\\n\"\n",
    "        \"All the conversations must be somehow diverse.\\n\"\n",
    "        \"I want only questions that ask the definition, not more. \\n\"\n",
    "        \"Each conversation will be formatted in a json list, of the form :\"\n",
    "        \"[\\n\"\n",
    "        \"  {\\n\"\n",
    "        \"     'role': 'user'',\\n\"\n",
    "        \"     'content': THE QUESTION\\n\"\n",
    "        \"  },\\n\"\n",
    "        \"  {\\n\"\n",
    "        \"     'role': 'assistant',\\n\"\n",
    "        \"     'content': THE ANSWER\\n\"\n",
    "        \"  }\\n\"\n",
    "        \"] \\n\"\n",
    "        \"Hence, the final result will look like : \\n\"\n",
    "        \"[\\n\"\n",
    "        \"   [\\n\"\n",
    "        \"       {\\n\"\n",
    "        \"           'role': 'user'',\\n\"\n",
    "        \"           'content': THE FIRST QUESTION\\n\"\n",
    "        \"       },\\n\"\n",
    "        \"       {\\n\"\n",
    "        \"           'role': 'assistant',\\n\"\n",
    "        \"           'content': THE ANSWER TO THE FIRST QUESTION\\n\"\n",
    "        \"       }\\n\"\n",
    "        \"   ], \\n\"\n",
    "        \"   [\\n\"\n",
    "        \"       {\\n\"\n",
    "        \"           'role': 'user'',\\n\"\n",
    "        \"           'content': THE SECOND QUESTION\\n\"\n",
    "        \"       },\\n\"\n",
    "        \"       {\\n\"\n",
    "        \"           'role': 'assistant',\\n\"\n",
    "        \"           'content': THE ANSWER TO THE SECOND QUESTION\\n\"\n",
    "        \"       }\\n\"\n",
    "        \"   ], \\n\"\n",
    "        \"   [\\n\"\n",
    "        \"       {\\n\"\n",
    "        \"           'role': 'user'',\\n\"\n",
    "        \"           'content': THE LAST QUESTION\\n\"\n",
    "        \"       },\\n\"\n",
    "        \"       {\\n\"\n",
    "        \"           'role': 'assistant',\\n\"\n",
    "        \"           'content': THE ANSWER TO THE LAST QUESTION\\n\"\n",
    "        \"       }\\n\"\n",
    "        \"   ] \\n\"\n",
    "        \"]\\n\"\n",
    "        f\"Keep it short. I remind you that you must give {n_conv} conversations.\\n\"\n",
    "        \"Your final answer must be only the raw json; no fioritures.\\n\"\n",
    "    )\n",
    "\n",
    "prompt = create_acronym_prompt(\n",
    "    2, acro=\"HPS\", definition=\"Herbs, Pasta, Spices: A cooking approach that heavily incorporates herbs and spices with pasta dishes.\"\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = generate(model=model_name, prompt=prompt).response\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the LLM could be anything, but we would rather want to check the format, to avoid future errors.\n",
    "We can give ollama.generate function a scheme, which will structure the output of the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import generate\n",
    "import json\n",
    "\n",
    "# We define a scheme for our list of Conversations, for type-checking.\n",
    "# The syntax for writing scheme is presented raw here, but usually\n",
    "# it is extracted from pydantic BaseModel subclasses.\n",
    "\n",
    "scheme_llm_output = {\n",
    "    \"$defs\": {\n",
    "        \"ConversationModel\": {\n",
    "            \"title\": \"ConversationModel\",\n",
    "            \"items\": {\n",
    "                \"$ref\": \"#/$defs/MessageModel\",\n",
    "            },\n",
    "            \"title\": \"Messages\",\n",
    "            \"type\": \"array\",\n",
    "        },\n",
    "        \"MessageModel\": {\n",
    "            \"properties\": {\n",
    "                \"role\": {\n",
    "                    \"enum\": [\"user\", \"assistant\"],\n",
    "                    \"title\": \"Role\",\n",
    "                    \"type\": \"string\",\n",
    "                },\n",
    "                \"content\": {\"title\": \"Content\", \"type\": \"string\"},\n",
    "            },\n",
    "            \"required\": [\"role\", \"content\"],\n",
    "            \"title\": \"MessageModel\",\n",
    "            \"type\": \"object\",\n",
    "        },\n",
    "    },\n",
    "    \"items\": {\"$ref\": \"#/$defs/ConversationModel\"},\n",
    "    \"title\": \"ConversationListModel\",\n",
    "    \"type\": \"array\"\n",
    "}\n",
    "\n",
    "structured_output = json.loads(generate(model=model_name, prompt=prompt, format=scheme_llm_output).response)\n",
    "\n",
    "print(structured_output)\n",
    "print(len(structured_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercises :__\n",
    "\n",
    "- change the prompt to get : \n",
    "\n",
    "    - more verbose answers, or more concise ones,\n",
    "\n",
    "    - answers in an other language,\n",
    "\n",
    "    - answers that specify the field of the acronym (e.g. cooking, science, ...) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training and test dataset\n",
    "\n",
    "We load our structured list of acronyms and their definitions, and we create a datasets of conversations for the training and test of the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "\n",
    "# loads list of acronyms and their definitions\n",
    "raw_data_dir = os.path.join(data_dir, \"acronym.json\")\n",
    "\n",
    "with open(raw_data_dir, \"rt\") as f:\n",
    "    raw_data = json.load(f)\n",
    "    \n",
    "n_acros = len(raw_data)\n",
    "print(f\"Example of dataset element : \\n {json.dumps(raw_data[random.randint(0, len(raw_data)-1)], indent=4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm # for progress bars\n",
    "\n",
    "train_dataset = []\n",
    "test_dataset = []\n",
    "n_convs_per_acro = 4\n",
    "\n",
    "for each_elem in tqdm(raw_data):\n",
    "    acronym = each_elem[\"acronym\"]\n",
    "    definition = each_elem[\"definition\"]\n",
    "    prompt = create_acronym_prompt(n_convs_per_acro, acronym, definition)\n",
    "    structured_output = json.loads(generate(model=model_name, prompt=prompt, format=scheme_llm_output).response)\n",
    "    if len(structured_output) < n_convs_per_acro:\n",
    "        print(f\"Skipping acronym {acronym}\")\n",
    "        continue\n",
    "\n",
    "    train_conv = structured_output[:n_convs_per_acro-1]\n",
    "    eval_conv = [structured_output[n_convs_per_acro-1]]\n",
    "\n",
    "    train_elem = {\n",
    "        \"acronym\": acronym,\n",
    "        \"ground_truth\": definition,\n",
    "        \"conversation\": train_conv,\n",
    "    }\n",
    "    test_elem = {\n",
    "        \"acronym\": acronym,\n",
    "        \"ground_truth\": definition,\n",
    "        \"conversation\": eval_conv,\n",
    "    }\n",
    "\n",
    "    train_dataset.append(train_elem)\n",
    "    test_dataset.append(test_elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[8] # example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we save the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = \"../bucket/fine_tuning_acronym/data/train_dataset.json\"\n",
    "test_data_dir = \"../bucket/fine_tuning_acronym/data/test_dataset.json\"\n",
    "\n",
    "# saves into a single json all training conversations\n",
    "with open(train_data_dir, \"wt\") as f:\n",
    "    json.dump(train_dataset, f, indent=4) # avoid indent param for bigger datasets\n",
    "\n",
    "# saves into a single json all test conversations\n",
    "with open(test_data_dir, \"wt\") as f:\n",
    "    json.dump(test_dataset, f, indent=4) # avoid indent param for bigger datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
